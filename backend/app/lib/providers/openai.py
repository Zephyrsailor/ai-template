import json
import re
import logging
from openai import AsyncOpenAI, APIError, APITimeoutError, RateLimitError
from typing import List, Dict, Optional, AsyncGenerator, Any, Union
from .base import BaseProvider, MessageDict
import asyncio
from ...domain.models.events import ModelEvent
from ...domain.constants import EventType, supports_function_calling

# åˆå§‹åŒ–logger
logger = logging.getLogger(__name__)

class OpenAIProvider(BaseProvider):
    """
    Async Provider for OpenAI and compatible APIs using the openai library.
    Focuses on core chat completion functionality.
    æ”¯æŒFunction Callingå’Œæ–‡æœ¬æ¨¡å¼çš„è‡ªåŠ¨åˆ‡æ¢ - å•å·¥å…·è°ƒç”¨æ¨¡å¼
    """
    def __init__(self, api_key: str, base_url: Optional[str] = None):
        if not api_key:
            raise ValueError("API key is required.")
        self.api_key = api_key
        self.base_url = base_url
        self._observation_queue = asyncio.Queue()
        # å·¥å…·åç§°æ˜ å°„ï¼šOpenAIæ ¼å¼ -> åŸå§‹åç§°
        self._tool_name_mapping = {}
        try:
            # Use AsyncOpenAI for FastAPI integration
            self.client = AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.base_url,
                timeout=60.0,
            )
            print(f"AsyncOpenAI client initialized. Target URL: {self.client.base_url}")
        except Exception as e:
            print(f"Error initializing AsyncOpenAI client: {e}")
            raise
    
    def supports_function_calling(self, model_id: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒFunction Calling"""
        return supports_function_calling(model_id)
    
    def _build_react_prompt(self, system_prompt: Optional[str], tools: List[Any]) -> str:
        """æ„å»ºReActæç¤ºè¯"""
        return super()._build_prompt(system_prompt, tools)
    
    def _clean_tool_name_for_openai(self, name: str) -> str:
        """æ¸…ç†å·¥å…·åç§°ï¼Œä½¿å…¶ç¬¦åˆOpenAI Function Callingæ ¼å¼è¦æ±‚"""
        # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦ä¸ºä¸‹åˆ’çº¿
        cleaned = re.sub(r'[^a-zA-Z0-9_-]', '_', name)
        # ç§»é™¤è¿ç»­çš„ä¸‹åˆ’çº¿
        cleaned = re.sub(r'_+', '_', cleaned)
        # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ä¸‹åˆ’çº¿
        cleaned = cleaned.strip('_')
        return cleaned
    
    def _convert_tools_to_openai_format(self, tools: List[Any]) -> List[Dict[str, Any]]:
        """å°†å·¥å…·è½¬æ¢ä¸ºOpenAI Function Callingæ ¼å¼"""
        openai_tools = []
        self._tool_name_mapping.clear()  # æ¸…ç©ºä¹‹å‰çš„æ˜ å°„
        
        for tool in tools:
            # å…¼å®¹å¤„ç†Toolå¯¹è±¡å’Œå­—å…¸æ ¼å¼
            if hasattr(tool, "name"):
                # å¤„ç†Toolå¯¹è±¡
                original_name = tool.name
                description = tool.description
                parameters = {
                    "type": "object",
                    "properties": {},
                    "required": []
                }
                
                for param in tool.parameters:
                    parameters["properties"][param.name] = {
                        "type": param.type or "string",
                        "description": param.description
                    }
                    if param.required:
                        parameters["required"].append(param.name)
            else:
                # å¤„ç†å­—å…¸æ ¼å¼
                original_name = tool.get("name", "")
                description = tool.get("description", "")
                parameters = tool.get("parameters", {})
            
            # æ¸…ç†å·¥å…·åç§°
            openai_name = self._clean_tool_name_for_openai(original_name)
            
            # å­˜å‚¨æ˜ å°„å…³ç³»
            self._tool_name_mapping[openai_name] = original_name
            
            openai_tool = {
                "type": "function",
                "function": {
                    "name": openai_name,
                    "description": description,
                    "parameters": parameters
                }
            }
            openai_tools.append(openai_tool)
        
        return openai_tools
                
    async def completions(
        self,
        messages: List[MessageDict],
        model_id: str,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Any]] = None,
        temperature: Optional[float] = 0.7,
        max_tokens: Optional[int] = 1024,
        stream: bool = True,
        **kwargs: Any
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æ‰§è¡Œå¸¦å·¥å…·é›†æˆçš„å¯¹è¯è¡¥å…¨ã€‚
        è‡ªåŠ¨æ£€æµ‹æ¨¡å‹èƒ½åŠ›ï¼Œæ”¯æŒFunction Callingæˆ–æ–‡æœ¬æ¨¡å¼ - å•å·¥å…·è°ƒç”¨æ¨¡å¼
        """
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒFunction Calling
            supports_fc = self.supports_function_calling(model_id)
            
            if supports_fc and tools:
                # ä½¿ç”¨Function Callingæ¨¡å¼
                async for event in self._completions_with_function_calling(
                    messages, model_id, system_prompt, tools, temperature, max_tokens, stream, **kwargs
                ):
                    yield event
            else:
                # ä½¿ç”¨æ–‡æœ¬æ¨¡å¼ï¼ˆReActï¼‰
                async for event in self._completions_with_text_mode(
                    messages, model_id, system_prompt, tools, temperature, max_tokens, stream, **kwargs
                ):
                    yield event
                    
        except Exception as e:
            error_msg = f"å¯¹è¯å¤„ç†é”™è¯¯: {str(e)}"
            print(f"è‡´å‘½é”™è¯¯: {error_msg}")
            yield ModelEvent(EventType.CONTENT, "å¾ˆæŠ±æ­‰ï¼Œæˆ‘åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†æŠ€æœ¯å›°éš¾ã€‚è¯·å°è¯•é‡æ–°æé—®æˆ–ç®€åŒ–æ‚¨çš„é—®é¢˜ã€‚")

    async def _completions_with_function_calling(
        self,
        messages: List[MessageDict],
        model_id: str,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Any]] = None,
        temperature: Optional[float] = 0.7,
        max_tokens: Optional[int] = 1024,
        stream: bool = True,
        **kwargs: Any
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """ä½¿ç”¨OpenAI Function Callingæ¨¡å¼ - å•å·¥å…·è°ƒç”¨æ¨¡å¼"""
        try:
            # å‡†å¤‡æ¶ˆæ¯
            conversation_messages = self._prepare_messages(messages, system_prompt)
            
            # è½¬æ¢å·¥å…·ä¸ºOpenAIæ ¼å¼
            openai_tools = self._convert_tools_to_openai_format(tools) if tools else None
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ¶ˆæ¯
            if len(conversation_messages) <= (1 if system_prompt else 0):
                error_msg = "æ²¡æœ‰æœ‰æ•ˆçš„ç”¨æˆ·/åŠ©æ‰‹æ¶ˆæ¯å¯å‘é€"
                print(f"é”™è¯¯: {error_msg}")
                yield ModelEvent(EventType.ERROR, {"error": error_msg})
                return
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            request_params = {
                "model": model_id,
                "messages": conversation_messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": stream,
                **kwargs
            }
            
            # æ·»åŠ å·¥å…·å‚æ•°
            if openai_tools:
                request_params["tools"] = openai_tools
                request_params["tool_choice"] = "auto"  # è®©æ¨¡å‹è‡ªåŠ¨å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·
            
            full_response = ""
            tool_calls = []
            model_response_error = False

            try:
                response_stream = await self.client.chat.completions.create(**request_params)
                
                async for chunk in response_stream:
                    if not chunk.choices:
                        continue
                        
                    delta = chunk.choices[0].delta
                    
                    # å¤„ç†æ€è€ƒå†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
                    reasoning_chunk = getattr(delta, 'reasoning_content', getattr(delta, 'thinking', None))
                    if reasoning_chunk:
                        yield ModelEvent(EventType.THINKING, reasoning_chunk)
                    
                    # å¤„ç†å¸¸è§„å†…å®¹
                    if delta.content:
                        full_response += delta.content
                        yield ModelEvent(EventType.CONTENT, delta.content)
                    
                    # å¤„ç†å·¥å…·è°ƒç”¨
                    if delta.tool_calls:
                        for tool_call in delta.tool_calls:
                            # ç¡®ä¿tool_callsåˆ—è¡¨è¶³å¤Ÿé•¿
                            while len(tool_calls) <= tool_call.index:
                                tool_calls.append({
                                    "id": "",
                                    "type": "function",
                                    "function": {"name": "", "arguments": ""}
                                })
                            
                            # æ›´æ–°å·¥å…·è°ƒç”¨ä¿¡æ¯
                            if tool_call.id:
                                tool_calls[tool_call.index]["id"] = tool_call.id
                            if tool_call.type:
                                tool_calls[tool_call.index]["type"] = tool_call.type
                            if tool_call.function:
                                if tool_call.function.name:
                                    tool_calls[tool_call.index]["function"]["name"] = tool_call.function.name
                                if tool_call.function.arguments:
                                    tool_calls[tool_call.index]["function"]["arguments"] += tool_call.function.arguments
                
            except (APITimeoutError, RateLimitError, APIError) as api_error:
                error_msg = f"API é”™è¯¯: {str(api_error)}"
                yield ModelEvent(EventType.ERROR, {"error": error_msg})
                print(f"APIé”™è¯¯: {error_msg}")
                model_response_error = True
                
            except Exception as e:
                error_msg = f"æµå¤„ç†é”™è¯¯: {str(e)}"
                yield ModelEvent(EventType.ERROR, {"error": error_msg})
                print(f"æµå¤„ç†é”™è¯¯: {error_msg}")
                model_response_error = True
            
            # å¦‚æœå‘ç”Ÿé”™è¯¯ï¼Œé€€å‡º
            if model_response_error:
                return
            
            # å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆå•å·¥å…·è°ƒç”¨æ¨¡å¼ï¼‰
            if tool_calls:
                # åªå¤„ç†ç¬¬ä¸€ä¸ªå·¥å…·è°ƒç”¨
                tool_call = tool_calls[0]
                try:
                    openai_function_name = tool_call["function"]["name"]
                    function_args = json.loads(tool_call["function"]["arguments"])
                    
                    # å°†OpenAIæ ¼å¼çš„å·¥å…·åè½¬æ¢å›åŸå§‹åç§°
                    original_tool_name = self._tool_name_mapping.get(openai_function_name, openai_function_name)
                    
                    # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
                    tool_call_data = {
                        "tool_name": original_tool_name,
                        "arguments": function_args
                    }
                    
                    yield ModelEvent(EventType.TOOL_CALL, json.dumps(tool_call_data))
                    
                except json.JSONDecodeError as e:
                    error_msg = f"å·¥å…·è°ƒç”¨å‚æ•°è§£æå¤±è´¥: {str(e)}"
                    yield ModelEvent(EventType.ERROR, {"error": error_msg})
                except Exception as e:
                    error_msg = f"å·¥å…·è°ƒç”¨å¤„ç†å¤±è´¥: {str(e)}"
                    yield ModelEvent(EventType.ERROR, {"error": error_msg})
                    
        except Exception as e:
            error_msg = f"Function Callingæ¨¡å¼é”™è¯¯: {str(e)}"
            print(f"Function Callingé”™è¯¯: {error_msg}")
            yield ModelEvent(EventType.ERROR, {"error": error_msg})

    async def _completions_with_text_mode(
        self,
        messages: List[MessageDict],
        model_id: str,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Any]] = None,
        temperature: Optional[float] = 0.7,
        max_tokens: Optional[int] = 1024,
        stream: bool = True,
        **kwargs: Any
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """ä½¿ç”¨æ–‡æœ¬æ¨¡å¼ï¼ˆReActæç¤ºè¯ï¼‰ - å•å·¥å…·è°ƒç”¨æ¨¡å¼"""
        try:
            # æ„å»ºsystem_promptï¼Œä¼˜å…ˆä½¿ç”¨toolsæ„å»ºprompt
            prompt = None
            if tools:
                prompt = self._build_react_prompt(system_prompt, tools)
            else:
                prompt = system_prompt

            # ä½¿ç”¨_prepare_messageså‡†å¤‡æ¶ˆæ¯
            conversation_messages = self._prepare_messages(messages, prompt)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ¶ˆæ¯
            if len(conversation_messages) <= (1 if prompt else 0):
                error_msg = "æ²¡æœ‰æœ‰æ•ˆçš„ç”¨æˆ·/åŠ©æ‰‹æ¶ˆæ¯å¯å‘é€"
                print(f"é”™è¯¯: {error_msg}")
                yield ModelEvent(EventType.ERROR, {"error": error_msg})
                return
            
            full_response = ""
            has_tool_call = False
            model_response_error = False

            try:
                response_stream = await self.client.chat.completions.create(
                    model=model_id,
                    messages=conversation_messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    stream=stream,
                    **kwargs
                )
                
                async for chunk in response_stream:
                    delta = getattr(chunk.choices[0], 'delta', None)                    
                    # å¤„ç†æ€è€ƒå†…å®¹
                    reasoning_chunk = getattr(delta, 'reasoning_content', getattr(delta, 'thinking', None))
                    if reasoning_chunk:
                        yield ModelEvent(EventType.THINKING, reasoning_chunk)
                        
                    # å¤„ç†å¸¸è§„å†…å®¹
                    content_chunk = getattr(delta, 'content', None)
                    if content_chunk:
                        full_response += content_chunk
                        
                        # æ£€æµ‹æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨ï¼ˆå•å·¥å…·è°ƒç”¨æ¨¡å¼ï¼‰
                        if "```json" in full_response and "{" in full_response and not has_tool_call:
                            has_tool_call = True
                        
                        # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¾“å‡ºå†…å®¹
                        if not has_tool_call:
                            yield ModelEvent(EventType.CONTENT, content_chunk)
                    
            except (APITimeoutError, RateLimitError, APIError) as api_error:
                error_msg = f"API é”™è¯¯: {str(api_error)}"
                yield ModelEvent(EventType.ERROR, {"error": error_msg})
                print(f"APIé”™è¯¯: {error_msg}")
                model_response_error = True
                
            except Exception as e:
                error_msg = f"æµå¤„ç†é”™è¯¯: {str(e)}"
                yield ModelEvent(EventType.ERROR, {"error": error_msg})
                print(f"æµå¤„ç†é”™è¯¯: {error_msg}")
                model_response_error = True
            
            # å¦‚æœå‘ç”Ÿé”™è¯¯æˆ–å“åº”ä¸ºç©ºï¼Œé€€å‡ºå¾ªç¯
            if model_response_error or not full_response:
                return
                        
            # å¤„ç†å·¥å…·è°ƒç”¨(å¦‚æœæœ‰)
            if has_tool_call:
                tool_call_data = self._parse_tool_call(full_response)
                if tool_call_data:
                    yield ModelEvent(EventType.TOOL_CALL, json.dumps(tool_call_data))
                
        except Exception as e:
            error_msg = f"æ–‡æœ¬æ¨¡å¼é”™è¯¯: {str(e)}"
            print(f"æ–‡æœ¬æ¨¡å¼é”™è¯¯: {error_msg}")
            yield ModelEvent(EventType.ERROR, {"error": error_msg})

    def _prepare_messages(self, messages: List[MessageDict], system_prompt: Optional[str]) -> List[Dict[str, str]]:
        """
        å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨ï¼Œæ·»åŠ ç³»ç»Ÿæç¤ºå¹¶å¤„ç†å ä½ç¬¦
        
        Args:
            messages: ç”¨æˆ·å’ŒåŠ©æ‰‹æ¶ˆæ¯åˆ—è¡¨
            system_prompt: å¯é€‰çš„ç³»ç»Ÿæç¤º
            
        Returns:
            å‡†å¤‡å¥½çš„æ¶ˆæ¯åˆ—è¡¨
        """
        request_messages: List[Dict[str, str]] = []
        
        # å¤„ç†ç³»ç»Ÿæç¤º
        if system_prompt:
            # å¦‚æœç³»ç»Ÿæç¤ºåŒ…å«{input}å ä½ç¬¦ï¼Œä½¿ç”¨æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯æ›¿æ¢
            if "{input}" in system_prompt:
                user_question = next((msg.get("content", "") for msg in reversed(messages) 
                                    if msg.get("role") == "user"), "")
                system_prompt = system_prompt.replace("{input}", user_question)
                
            request_messages.append({"role": "system", "content": system_prompt})
            
        # å¤„ç†ç”¨æˆ·å’ŒåŠ©æ‰‹æ¶ˆæ¯
        for msg in messages:
            if isinstance(msg, dict) and \
               msg.get('role') in ['user', 'assistant', 'tool'] and \
               isinstance(msg.get('content'), str) and \
               msg['content'].strip():
                
                # æ„å»ºæ¶ˆæ¯å¯¹è±¡
                message = {"role": msg['role'], "content": msg['content']}
                
                # å¦‚æœæ˜¯toolè§’è‰²çš„æ¶ˆæ¯ï¼Œå¿…é¡»åŒ…å«tool_call_id
                if msg.get('role') == 'tool':
                    if 'tool_call_id' in msg:
                        message['tool_call_id'] = msg['tool_call_id']
                    else:
                        # è·³è¿‡æ²¡æœ‰tool_call_idçš„toolæ¶ˆæ¯
                        print(f"è·³è¿‡ç¼ºå°‘tool_call_idçš„toolæ¶ˆæ¯: {msg.get('content', '')[:50]}...")
                        continue
                
                request_messages.append(message)
                
        return request_messages
    
    def _parse_tool_call(self, response: str) -> Dict[str, Any]:
        """
        è§£æå“åº”ä¸­çš„å·¥å…·è°ƒç”¨JSON - å•å·¥å…·è°ƒç”¨æ¨¡å¼
        
        Args:
            response: æ¨¡å‹çš„å“åº”æ–‡æœ¬
            
        Returns:
            å·¥å…·è°ƒç”¨çš„å­—å…¸ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›None
        """
        # æŸ¥æ‰¾JSONä»£ç å—
        tool_match = re.search(r"```json\s*\n(.*?)\n```", response, re.DOTALL)
        if not tool_match:
            return None
        
        try:
            json_str = tool_match.group(1).strip()
            tool_call = json.loads(json_str)
            
            # éªŒè¯å·¥å…·è°ƒç”¨æ ¼å¼
            if isinstance(tool_call, dict) and "tool_name" in tool_call:
                # ğŸ”¥ ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨å·¥å…·åç§°ï¼Œä¸éœ€è¦å¤æ‚æ˜ å°„
                return {
                    "tool_name": tool_call.get("tool_name"),
                    "arguments": tool_call.get("arguments", {})
                }
            else:
                print(f"å·¥å…·è°ƒç”¨æ ¼å¼ä¸æ­£ç¡®: {json_str}")
                return None
                
        except json.JSONDecodeError as e:
            print(f"å·¥å…·è°ƒç”¨JSONè§£æå¤±è´¥: {e}")
            return None
    
    def _build_prompt(self, system_prompt: Optional[str], tools: List[Any]) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        prompt = system_prompt or ""
        if tools:
            prompt += "\n\n" + "\n".join([f"{tool.name}: {tool.description}" for tool in tools])
        return prompt

    async def list_models(self) -> List[str]:
        """
        è·å–OpenAI APIå¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        """
        try:
            models_response = await self.client.models.list()
            models = []
            
            # è¿‡æ»¤å‡ºèŠå¤©æ¨¡å‹ï¼ˆæ’é™¤åµŒå…¥ã€å›¾åƒç­‰æ¨¡å‹ï¼‰
            chat_model_prefixes = ['gpt-', 'o1-', 'text-davinci', 'text-curie', 'text-babbage', 'text-ada']
            
            for model in models_response.data:
                model_id = model.id
                # åªåŒ…å«èŠå¤©ç›¸å…³çš„æ¨¡å‹
                if any(model_id.startswith(prefix) for prefix in chat_model_prefixes):
                    models.append(model_id)
            
            # æŒ‰åç§°æ’åº
            models.sort()
            return models
            
        except Exception as e:
            logger.error(f"è·å–OpenAIæ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤æ¨¡å‹åˆ—è¡¨ä½œä¸ºåå¤‡
            return ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "gpt-4", "gpt-3.5-turbo", "o1-preview", "o1-mini"]

    async def check_model_availability(self, model_id: str) -> bool:
        """
        æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨OpenAIä¸­å¯ç”¨
        """
        try:
            available_models = await self.list_models()
            return model_id in available_models
        except Exception as e:
            logger.error(f"æ£€æŸ¥OpenAIæ¨¡å‹å¯ç”¨æ€§å¤±è´¥: {str(e)}")
            return False

