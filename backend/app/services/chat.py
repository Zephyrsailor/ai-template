"""
èŠå¤©æœåŠ¡ - é‡æ„ç‰ˆæœ¬
å°†åŸæœ‰çš„è¶…é•¿æ–¹æ³•æ‹†åˆ†ä¸ºå¤šä¸ªèŒè´£å•ä¸€çš„æ–¹æ³•ï¼Œæé«˜ä»£ç å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§
"""
import json
import uuid
import asyncio
from typing import List, Dict, Any, Optional, AsyncGenerator, Union, Tuple
from dataclasses import dataclass

from ..domain.schemas.tools import Tool
from ..core.config import get_settings, get_provider
from ..core.logging import get_logger
from ..core.errors import ServiceException
from ..core.messages import get_message, MessageKeys
from ..core.constants import ChatConstants, APIConstants
from ..domain.constants import EventType, MessageRole
from ..domain.schemas.chat import StreamEvent, ChatRequest
from ..domain.models.events import ModelEvent
from fastapi import HTTPException
from ..services.knowledge import KnowledgeService
from ..services.mcp import MCPService
from ..services.conversation import ConversationService
from ..services.user_llm_config import UserLLMConfigService
from ..domain.models.user import User
from ..services.search import search_web
from ..utils.context_integration import ChatContextHelper

logger = get_logger(__name__)

@dataclass
class ChatContext:
    """èŠå¤©ä¸Šä¸‹æ–‡æ•°æ®ç±»"""
    knowledge_context: Optional[str] = None
    web_search_context: Optional[str] = None
    tools: Optional[List[Tool]] = None
    mcp_servers: List[Any] = None
    conversation_id: Optional[str] = None
    messages: List[Dict[str, str]] = None

@dataclass
class ChatIteration:
    """èŠå¤©è¿­ä»£çŠ¶æ€"""
    has_final_answer: bool = False
    iteration: int = 0
    max_iterations: int = ChatConstants.MAX_REACT_ITERATIONS
    collected_thinking: str = ""
    collected_content: str = ""
    collected_tool_calls: List[Dict] = None

class ChatContextBuilder:
    """èŠå¤©ä¸Šä¸‹æ–‡æ„å»ºå™¨"""
    
    def __init__(self, knowledge_service: KnowledgeService, mcp_service: Optional[MCPService], current_user: Optional[User]):
        self.knowledge_service = knowledge_service
        self.mcp_service = mcp_service
        self.current_user = current_user
    
    async def build_knowledge_context(self, request: ChatRequest) -> Optional[str]:
        """æ„å»ºçŸ¥è¯†åº“ä¸Šä¸‹æ–‡"""
        if not request.knowledge_base_ids:
            return None
            
        try:
            knowledge_results = await self.knowledge_service.query_multiple(
                request.knowledge_base_ids,
                request.message,
                top_k=ChatConstants.KNOWLEDGE_TOP_K,
                current_user=self.current_user
            )
            if knowledge_results:
                # å­˜å‚¨ç»“æœä¾›åç»­å¼•ç”¨ä½¿ç”¨
                self.knowledge_results = knowledge_results
                return self.knowledge_service.format_knowledge_results(knowledge_results)
        except Exception as e:
            logger.error(f"çŸ¥è¯†åº“æŸ¥è¯¢å¤±è´¥: {str(e)}", exc_info=True)
        
        return None
    
    async def build_web_search_context(self, request: ChatRequest) -> Optional[str]:
        """æ„å»ºç½‘ç»œæœç´¢ä¸Šä¸‹æ–‡"""
        if not request.use_web_search:
            return None
            
        try:
            logger.info(f"æ‰§è¡Œç½‘ç»œæœç´¢ï¼š{request.message}")
            search_results = await search_web(request.message)
            
            if search_results and not any("error" in r for r in search_results):
                # å­˜å‚¨ç»“æœä¾›åç»­å¼•ç”¨ä½¿ç”¨
                self.search_results = search_results
                web_context_parts = ["### ç½‘ç»œæœç´¢ç»“æœ:\n"]
                
                for result in search_results:
                    if result.get("content"):
                        source_info = f"æ¥æºï¼š{result['title']}\né“¾æ¥ï¼š{result['url']}\n"
                        content_preview = result["content"][:ChatConstants.MAX_CONTENT_PREVIEW] + "..." if len(result["content"]) > ChatConstants.MAX_CONTENT_PREVIEW else result["content"]
                        web_context_parts.append(f"{source_info}\n{content_preview}\n")
                    elif result.get("snippet"):
                        source_info = f"æ¥æºï¼š{result['title']}\né“¾æ¥ï¼š{result['url']}\n"
                        web_context_parts.append(f"{source_info}\n{result['snippet']}\n")
                
                logger.info("ç½‘ç»œæœç´¢æˆåŠŸï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡")
                return "\n".join(web_context_parts)
        except Exception as e:
            logger.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
        
        return None
    
    async def build_mcp_tools(self, request: ChatRequest) -> Tuple[Optional[List[Tool]], List[Any]]:
        """æ„å»ºMCPå·¥å…·å’ŒæœåŠ¡å™¨åˆ—è¡¨"""
        if not request.mcp_server_ids or not self.mcp_service:
            return None, []
            
        try:
            mcp_servers = []
            mcp_server_names = []
            for server_id in request.mcp_server_ids:
                server = await self.mcp_service.get_server(server_id, self.current_user.id)
                if server:
                    mcp_servers.append(server)
                    mcp_server_names.append(server.name)
            if mcp_servers:
                request.use_tools = True
                tools = await self.mcp_service.get_user_tools(self.current_user.id, request.mcp_server_ids)
                return tools, mcp_servers
        except Exception as e:
            logger.error(f"MCPæœåŠ¡å™¨/å·¥å…·å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
            raise HTTPException(
                status_code=APIConstants.HTTP_INTERNAL_ERROR, 
                detail=get_message(MessageKeys.MCP_TOOL_CALL_FAILED, error=str(e))
            )
        
        return None, []

class ConversationManager:
    """ä¼šè¯ç®¡ç†å™¨"""
    
    def __init__(self, conversation_service: ConversationService, current_user: Optional[User]):
        self.conversation_service = conversation_service
        self.current_user = current_user
    
    async def get_or_create_conversation(self, request: ChatRequest) -> Tuple[Any, str]:
        """è·å–æˆ–åˆ›å»ºä¼šè¯"""
        if not self.current_user:
            logger.warning("No current user, cannot create conversation")
            return None, None
            
        try:
            conversation = None
            conversation_id = request.conversation_id
            
            if conversation_id:
                conversation = await self.conversation_service.get_conversation(self.current_user.id, conversation_id)
                if not conversation:
                    logger.info(f"Creating new conversation for existing ID {conversation_id}")
                    conversation = await self.conversation_service.create_conversation(
                        user_id=self.current_user.id,
                        title=request.conversation_title or "æ–°ä¼šè¯"
                    )
                    conversation_id = conversation.id
                    # æ‰‹åŠ¨æäº¤äº‹åŠ¡ï¼Œç¡®ä¿ä¼šè¯ç«‹å³ä¿å­˜
                    await self.conversation_service.session.commit()
                    logger.info(f"New conversation created and committed: {conversation_id}")
            else:
                logger.info("Creating new conversation")
                conversation = await self.conversation_service.create_conversation(
                    user_id=self.current_user.id,
                    title=request.conversation_title or "æ–°ä¼šè¯"
                )
                conversation_id = conversation.id
                # æ‰‹åŠ¨æäº¤äº‹åŠ¡ï¼Œç¡®ä¿ä¼šè¯ç«‹å³ä¿å­˜
                await self.conversation_service.session.commit()
                logger.info(f"New conversation created and committed: {conversation_id}")
            
            logger.info(f"Conversation ready: {conversation_id}")
            return conversation, conversation_id
                    
        except Exception as e:
            logger.error(f"Failed to create conversation: {str(e)}", exc_info=True)
            await self.conversation_service.session.rollback()
            raise
    
    async def add_user_message(self, request: ChatRequest, conversation_id: str):
        """æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°ä¼šè¯"""
        if not self.current_user:
            logger.warning("No current user, skipping user message save")
            return
            
        try:
            message_metadata = {}
            if request.knowledge_base_ids:
                message_metadata["knowledge_base_ids"] = request.knowledge_base_ids
            if request.mcp_server_ids:
                message_metadata["mcp_server_ids"] = request.mcp_server_ids
            if request.use_web_search:
                message_metadata["web_search"] = True
                
            logger.info(f"Saving user message to conversation {conversation_id}")
            
            await self.conversation_service.add_message(
                user_id=self.current_user.id,
                conversation_id=conversation_id,
                role="user",
                content=request.message,
                metadata=message_metadata
            )
            
            # æ‰‹åŠ¨æäº¤äº‹åŠ¡ï¼Œç¡®ä¿ç”¨æˆ·æ¶ˆæ¯ç«‹å³ä¿å­˜
            await self.conversation_service.session.commit()
            logger.info("User message saved and committed successfully")
                    
        except Exception as e:
            logger.error(f"Failed to save user message: {str(e)}", exc_info=True)
            await self.conversation_service.session.rollback()
            raise
    
    async def add_assistant_message(self, request: ChatRequest, conversation_id: str, 
                                   content: str, metadata: Dict[str, Any] = None, 
                                   thinking: str = None, tool_calls: List[Dict[str, Any]] = None):
        """æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°ä¼šè¯"""
        if not self.current_user:
            logger.warning("No current user, skipping assistant message save")
            return
            
        try:
            logger.info(f"Saving assistant message to conversation {conversation_id}")
            
            await self.conversation_service.add_message(
                user_id=self.current_user.id,
                conversation_id=conversation_id,
                role="assistant",
                content=content,
                metadata=metadata,
                thinking=thinking,
                tool_calls=tool_calls
            )
            
            # æ‰‹åŠ¨æäº¤äº‹åŠ¡ï¼Œç¡®ä¿åŠ©æ‰‹æ¶ˆæ¯ç«‹å³ä¿å­˜
            await self.conversation_service.session.commit()
            logger.info("Assistant message saved and committed successfully")
                    
        except Exception as e:
            logger.error(f"Failed to save assistant message: {str(e)}", exc_info=True)
            await self.conversation_service.session.rollback()
            raise

class ChatService:
    """é‡æ„åçš„èŠå¤©æœåŠ¡"""
    
    def __init__(self, 
                knowledge_service: KnowledgeService,
                mcp_service: Optional[MCPService],
                current_user: Optional[User],
                conversation_service: ConversationService,
                user_llm_config_service: UserLLMConfigService):
        """åˆå§‹åŒ–èŠå¤©æœåŠ¡"""
        self.settings = get_settings()
        self.provider = get_provider()
        self.knowledge_service = knowledge_service
        self.mcp_service = mcp_service
        self.current_user = current_user
        self.conversation_service = conversation_service
        self.user_llm_config_service = user_llm_config_service
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.context_builder = ChatContextBuilder(knowledge_service, mcp_service, current_user)
        self.conversation_manager = ConversationManager(conversation_service, current_user)
        
        # åˆå§‹åŒ–ç»“æœå­˜å‚¨
        self.knowledge_results = None
        self.search_results = None
        
        logger.info(f"èŠå¤©æœåŠ¡å·²åˆ›å»ºï¼Œä½¿ç”¨æä¾›å•†: {self.settings.LLM_PROVIDER}")

    async def chat_stream(self, request: ChatRequest, stop_key: Optional[str] = None) -> AsyncGenerator[StreamEvent, None]:
        """
        ä¸»è¦çš„èŠå¤©æµæ–¹æ³• - é‡æ„åçš„ç®€æ´ç‰ˆæœ¬
        """
        try:
            # 1. å‡†å¤‡èŠå¤©ä¸Šä¸‹æ–‡
            context = await self._prepare_context(request, stop_key)
            if not context:  # å¦‚æœè¢«åœæ­¢
                return
            
            # 2. è®¾ç½®ä¼šè¯
            conversation, conversation_id = await self.conversation_manager.get_or_create_conversation(request)
            if not conversation or not conversation_id:
                logger.error("Failed to create or get conversation")
                yield StreamEvent(type=EventType.ERROR, data={
                    "error": "æ— æ³•åˆ›å»ºä¼šè¯",
                    "stage": "conversation_setup"
                })
                return
            
            await self.conversation_manager.add_user_message(request, conversation_id)
            context.conversation_id = conversation_id
            
            # 3. æ‰§è¡ŒèŠå¤©å¾ªç¯
            async for event in self._execute_chat_loop(request, context, stop_key):
                yield event
                
            # 4. å‘é€ä¼šè¯åˆ›å»ºäº‹ä»¶ï¼ˆå¦‚æœæ˜¯æ–°ä¼šè¯ï¼‰
            if request.conversation_id is None:
                yield StreamEvent(type=EventType.CONVERSATION_CREATED, data=conversation_id)
                
            # 5. å‘é€å¼•ç”¨ä¿¡æ¯
            async for event in self._send_references(request, context):
                yield event
                
        except Exception as e:
            logger.error(f"èŠå¤©å¤„ç†é”™è¯¯: {str(e)}", exc_info=True)
            yield StreamEvent(type=EventType.ERROR, data={
                "error": get_message(MessageKeys.CHAT_FAILED, error=str(e)),
                "stage": "chat_processing"
            })

    async def _prepare_context(self, request: ChatRequest, stop_key: Optional[str]) -> Optional[ChatContext]:
        """å‡†å¤‡èŠå¤©ä¸Šä¸‹æ–‡"""
        context = ChatContext()
        
        # æ£€æŸ¥åœæ­¢ä¿¡å·çš„è¾…åŠ©å‡½æ•°
        def is_stopped() -> bool:
            if not stop_key:
                return False
            from ..api.routes.chat import _stop_signals
            return _stop_signals.get(stop_key, False)
        
        # 1. æ„å»ºçŸ¥è¯†åº“ä¸Šä¸‹æ–‡
        context.knowledge_context = await self.context_builder.build_knowledge_context(request)
        if is_stopped():
            return None
        
        # 2. æ„å»ºç½‘ç»œæœç´¢ä¸Šä¸‹æ–‡
        context.web_search_context = await self.context_builder.build_web_search_context(request)
        if is_stopped():
            return None
        
        # 3. æ„å»ºMCPå·¥å…·
        context.tools, context.mcp_servers = await self.context_builder.build_mcp_tools(request)
        if is_stopped():
            return None
        
        # 4. å‡†å¤‡æ¶ˆæ¯
        context.messages = await self._prepare_messages(request, context)
        
        return context

    async def _prepare_messages(self, request: ChatRequest, context: ChatContext) -> List[Dict[str, str]]:
        """å‡†å¤‡èŠå¤©æ¶ˆæ¯ï¼ˆé›†æˆä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼‰"""
        context_parts = []
        
        # ç½‘ç»œæœç´¢ç»“æœä¼˜å…ˆ
        if context.web_search_context:
            context_parts.append(context.web_search_context)
            
        if context.knowledge_context:
            context_parts.append(context.knowledge_context)
        
        # å¢å¼ºç”¨æˆ·æ¶ˆæ¯
        user_message = request.message
        if context_parts:
            context_text = "\n\n".join(context_parts)
            user_message = f"{context_text}\n\nç”¨æˆ·é—®é¢˜: {request.message}"
        
        # å‡†å¤‡æ¶ˆæ¯å†å²
        messages = []
        if request.history:
            messages.extend(request.history)
        
        messages.append({"role": "user", "content": user_message})
        
        # ğŸ”¥ é›†æˆä¸Šä¸‹æ–‡ä¼˜åŒ–å™¨
        try:
            # è·å–ç”¨æˆ·çš„context_lengthé…ç½®
            context_length = await self._get_user_context_length(request)
            logger.info(f"è·å–åˆ°ç”¨æˆ·context_length: {context_length}, æ¨¡å‹: {request.model_id}")
            
            # åˆ›å»ºæ€»ç»“å‡½æ•°ï¼ˆå¦‚æœç”¨æˆ·æœ‰LLMé…ç½®ï¼‰
            summarize_func = None
            if self.current_user:
                summarize_func = await self._create_summarize_function()
            
            # æ™ºèƒ½ä¼˜åŒ–ä¸Šä¸‹æ–‡ - ä½¿ç”¨context_lengthè€Œä¸æ˜¯max_tokens
            optimized_messages, stats = ChatContextHelper.prepare_optimized_messages(
                messages=messages,
                max_tokens=context_length,  # è¿™é‡Œä½¿ç”¨context_lengthè¿›è¡Œä¸Šä¸‹æ–‡çª—å£ç®¡ç†
                knowledge_context=context.knowledge_context,
                web_context=context.web_search_context,
                tools_context=None,  # å·¥å…·ä¸Šä¸‹æ–‡åœ¨åç»­å¤„ç†
                summarize_func=summarize_func,
                conversation_id=context.conversation_id,
                user_preference="balanced"  # é»˜è®¤å¹³è¡¡ç­–ç•¥
            )
            
            # è®°å½•ä¼˜åŒ–ç»Ÿè®¡
            if stats.get("optimization_applied"):
                logger.info(f"ä¸Šä¸‹æ–‡ä¼˜åŒ–: ç­–ç•¥={stats.get('strategy_chosen')}, "
                          f"ç§»é™¤{stats.get('messages_removed', 0)}æ¡æ¶ˆæ¯, "
                          f"èŠ‚çœ{stats.get('tokens_saved', 0)}tokens, "
                          f"å‹ç¼©æ¯”ä¾‹={stats.get('compression_ratio', 0):.1%}")
                
                # å‘é€ä¼˜åŒ–é€šçŸ¥ç»™ç”¨æˆ·ï¼ˆå¯é€‰ï¼‰
                if stats.get('summary_generated'):
                    logger.info(f"ç”Ÿæˆå†å²å¯¹è¯æ€»ç»“ï¼ŒåŸå› : {stats.get('strategy_reason', 'æœªçŸ¥')}")
            
            context.messages = optimized_messages
            return optimized_messages
            
        except Exception as e:
            logger.error(f"ä¸Šä¸‹æ–‡ä¼˜åŒ–å¤±è´¥: {str(e)}, ä½¿ç”¨åŸå§‹æ¶ˆæ¯")
            context.messages = messages
            return messages
    
    async def _get_user_context_length(self, request: ChatRequest) -> int:
        """è·å–ç”¨æˆ·çš„context_lengthé…ç½®ï¼Œç”¨äºä¸Šä¸‹æ–‡çª—å£ä¼˜åŒ–"""
        try:
            if not self.current_user:
                logger.info("æ²¡æœ‰å½“å‰ç”¨æˆ·ï¼Œè¿”å›é»˜è®¤å€¼32768")
                return 32768  # é»˜è®¤å€¼
            
            # è·å–ç”¨æˆ·LLMé…ç½®
            user_configs = await self.user_llm_config_service.list_configs(self.current_user.id)
            logger.info(f"è·å–åˆ°ç”¨æˆ·é…ç½®æ•°é‡: {len(user_configs)}")
            
            # æŸ¥æ‰¾åŒ¹é…çš„é…ç½®
            user_config = None
            if request.model_id:
                logger.info(f"æŸ¥æ‰¾æ¨¡å‹ {request.model_id} çš„é…ç½®")
                for config in user_configs:
                    if config.model_name == request.model_id:
                        user_config = config
                        logger.info(f"æ‰¾åˆ°åŒ¹é…çš„é…ç½®: {config.model_name}")
                        break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
            if not user_config:
                logger.info("æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„é…ç½®ï¼ŒæŸ¥æ‰¾é»˜è®¤é…ç½®")
                for config in user_configs:
                    if config.is_default:
                        user_config = config
                        logger.info(f"æ‰¾åˆ°é»˜è®¤é…ç½®: {config.model_name}")
                        break
                
                if not user_config and user_configs:
                    user_config = user_configs[0]
                    logger.info(f"ä½¿ç”¨ç¬¬ä¸€ä¸ªé…ç½®: {user_config.model_name}")
            
            # ä»é…ç½®ä¸­è·å–context_length
            if user_config and hasattr(user_config, 'context_length') and user_config.context_length:
                logger.info(f"ä»ç”¨æˆ·é…ç½®è·å–context_length: {user_config.context_length}")
                return user_config.context_length
            
            logger.info("ç”¨æˆ·é…ç½®ä¸­æ²¡æœ‰context_lengthï¼Œä½¿ç”¨æ¨¡å‹æ¨æ–­")
            # æ ¹æ®æ¨¡å‹åç§°æ¨æ–­é»˜è®¤çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°
            model_name = request.model_id or "gpt-4"
            logger.info(f"æ¨æ–­context_length: æ¨¡å‹åç§°='{model_name}'")
            
            if "gpt-4" in model_name.lower():
                if "turbo" in model_name.lower() or "o1" in model_name.lower():
                    logger.info(f"åŒ¹é…GPT-4 Turbo/o1ï¼Œè¿”å›128000")
                    return 128000  # GPT-4 Turbo / o1
                logger.info(f"åŒ¹é…æ ‡å‡†GPT-4ï¼Œè¿”å›8192")
                return 8192    # æ ‡å‡†GPT-4
            elif "gpt-3.5" in model_name.lower():
                if "16k" in model_name.lower():
                    return 16384
                return 4096
            elif "deepseek" in model_name.lower():
                return 32768   # DeepSeekæ¨¡å‹é€šå¸¸æ”¯æŒè¾ƒå¤§ä¸Šä¸‹æ–‡
            elif "claude" in model_name.lower():
                if "3-5" in model_name.lower():
                    return 200000  # Claude 3.5
                elif "3" in model_name.lower():
                    return 200000  # Claude 3
                return 100000  # å…¶ä»–Claudeæ¨¡å‹
            elif "gemini" in model_name.lower():
                if "pro" in model_name.lower():
                    return 128000  # Gemini Pro
                return 32768
            elif "llama" in model_name.lower() or "qwen" in model_name.lower():
                logger.info(f"åŒ¹é…llama/qwenæ¨¡å‹: {model_name}")
                if "32b" in model_name.lower() or "70b" in model_name.lower():
                    logger.info(f"åŒ¹é…å¤§å‹æ¨¡å‹(32b/70b)ï¼Œè¿”å›131072")
                    return 131072  # å¤§å‹æ¨¡å‹æ”¯æŒæ›´å¤§ä¸Šä¸‹æ–‡ (128K)
                elif "14b" in model_name.lower() or "72b" in model_name.lower():
                    logger.info(f"åŒ¹é…ä¸­å‹æ¨¡å‹(14b/72b)ï¼Œè¿”å›65536")
                    return 65536   # ä¸­å‹æ¨¡å‹ (64K)
                else:
                    logger.info(f"åŒ¹é…å°å‹æ¨¡å‹ï¼Œè¿”å›32768")
                    return 32768   # å°å‹æ¨¡å‹ (32K)
            else:
                logger.info(f"æœªåŒ¹é…ä»»ä½•æ¨¡å‹ç±»å‹ï¼Œè¿”å›é»˜è®¤å€¼32768")
                return 32768   # é»˜è®¤å€¼
                
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·context_lengthå¤±è´¥: {str(e)}")
            return 32768
    
    async def _get_user_max_tokens(self, request: ChatRequest) -> int:
        """è·å–ç”¨æˆ·çš„max_tokensé…ç½®ï¼Œç”¨äºå•æ¬¡ç”Ÿæˆ"""
        try:
            if not self.current_user:
                return 4096  # é»˜è®¤å€¼
            
            # è·å–ç”¨æˆ·LLMé…ç½®
            user_configs = await self.user_llm_config_service.list_configs(self.current_user.id)
            
            # æŸ¥æ‰¾åŒ¹é…çš„é…ç½®
            user_config = None
            if request.model_id:
                for config in user_configs:
                    if config.model_name == request.model_id:
                        user_config = config
                        break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
            if not user_config:
                for config in user_configs:
                    if config.is_default:
                        user_config = config
                        break
                
                if not user_config and user_configs:
                    user_config = user_configs[0]
            
            # ä»é…ç½®ä¸­è·å–max_tokens
            if user_config and hasattr(user_config, 'max_tokens') and user_config.max_tokens:
                return user_config.max_tokens
            
            # æ ¹æ®æ¨¡å‹åç§°æ¨æ–­é»˜è®¤å€¼
            model_name = request.model_id or "gpt-4"
            if "gpt-4" in model_name.lower():
                return 8192
            elif "gpt-3.5" in model_name.lower():
                return 4096
            elif "deepseek" in model_name.lower():
                return 8192
            elif "claude" in model_name.lower():
                return 8192
            elif "gemini" in model_name.lower():
                return 8192
            elif "llama" in model_name.lower() or "qwen" in model_name.lower():
                return 4096  # Ollamaæ¨¡å‹
            else:
                return 4096  # é»˜è®¤å€¼
                
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·max_tokenså¤±è´¥: {str(e)}")
            return 4096
    
    async def _create_summarize_function(self):
        """åˆ›å»ºæ€»ç»“å‡½æ•°"""
        try:
            # è·å–ç”¨æˆ·çš„LLM Provider
            user_provider, user_model = await self._get_user_provider(
                self.current_user.id if self.current_user else None
            )
            
            async def summarize_func(prompt: str) -> str:
                """ä½¿ç”¨ç”¨æˆ·çš„LLMé…ç½®è¿›è¡Œæ€»ç»“"""
                try:
                    messages = [
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹æ€»ç»“åŠ©æ‰‹ï¼Œæ“…é•¿æå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆç®€æ´å‡†ç¡®çš„æ€»ç»“ã€‚"},
                        {"role": "user", "content": prompt}
                    ]
                    
                    response = ""
                    async for chunk in user_provider.completions(
                        model_id=user_model,
                        messages=messages,
                        max_tokens=1000,
                        temperature=0.3
                    ):
                        # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†ModelEventå¯¹è±¡
                        if hasattr(chunk, 'type') and hasattr(chunk, 'data'):
                            if chunk.type == "content":
                                response += chunk.data
                        elif isinstance(chunk, dict) and chunk.get("type") == "content":
                            response += chunk.get("data", "")
                    
                    return response.strip()
                    
                except Exception as e:
                    logger.error(f"LLMæ€»ç»“å¤±è´¥: {str(e)}")
                    return f"æ€»ç»“ç”Ÿæˆå¤±è´¥: {str(e)}"
            
            return summarize_func
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ€»ç»“å‡½æ•°å¤±è´¥: {str(e)}")
            return None

    async def _execute_chat_loop(self, request: ChatRequest, context: ChatContext, stop_key: Optional[str]) -> AsyncGenerator[StreamEvent, None]:
        """æ‰§è¡ŒèŠå¤©å¾ªç¯"""
        iteration_state = ChatIteration()
        iteration_state.collected_tool_calls = []
        
        # è·å–ç”¨æˆ·ç‰¹å®šçš„Providerå’Œæ¨¡å‹
        user_provider, user_model = await self._get_user_provider(
            self.current_user.id if self.current_user else None,
            request.model_id
        )
        
        logger.info(f"å¼€å§‹ReActæ¨¡å¼èŠå¤©ï¼Œæ¨¡å‹: {user_model}")
        
        while not iteration_state.has_final_answer and iteration_state.iteration < iteration_state.max_iterations:
            if self._is_stopped(stop_key):
                return
            
            # æ‰§è¡Œå•æ¬¡è¿­ä»£
            async for event in self._execute_single_iteration(
                request, context, iteration_state, user_provider, user_model, stop_key
            ):
                yield event
            
            iteration_state.iteration += 1

    async def _execute_single_iteration(self, request: ChatRequest, context: ChatContext, 
                                      iteration_state: ChatIteration, user_provider, user_model: str, 
                                      stop_key: Optional[str]) -> AsyncGenerator[StreamEvent, None]:
        """æ‰§è¡Œå•æ¬¡èŠå¤©è¿­ä»£ï¼ˆæ·»åŠ å·¥å…·è°ƒç”¨å®‰å…¨æ£€æŸ¥ï¼‰"""
        has_tool_call = False
        tool_call_json = None        
        
        # é‡ç½®æœ¬è½®çŠ¶æ€
        iteration_state.collected_thinking = ""
        iteration_state.collected_content = ""
        
        # è·å–ç”¨æˆ·é…ç½®çš„max_tokens
        user_max_tokens = await self._get_user_max_tokens(request)
        
        # è°ƒç”¨æ¨¡å‹
        async for event in user_provider.completions(
            messages=context.messages,
            model_id=user_model,
            system_prompt=request.system_prompt,
            tools=context.tools,
            temperature=request.temperature,
            max_tokens=user_max_tokens,  # ä½¿ç”¨ç”¨æˆ·é…ç½®çš„max_tokens
            stream=request.stream
        ):
            # æ£€æŸ¥åœæ­¢ä¿¡å·
            if self._is_stopped(stop_key):
                # ä¿å­˜å½“å‰çŠ¶æ€å¹¶é€€å‡º
                await self._handle_stop_signal(request, context, iteration_state)
                return
                
            # æ”¶é›†ä¸åŒç±»å‹çš„å†…å®¹
            if event.type == EventType.THINKING:
                iteration_state.collected_thinking += event.data
            elif event.type == EventType.CONTENT:
                iteration_state.collected_content += event.data
            elif event.type == EventType.TOOL_CALL:
                has_tool_call = True
                tool_call_json = self._process_tool_call(event.data)
                event.data = tool_call_json
            
            yield StreamEvent(type=event.type, data=event.data)
        
        # å¤„ç†å·¥å…·è°ƒç”¨æˆ–ç»“æŸå¯¹è¯
        if has_tool_call:
            async for tool_event in self._handle_tool_call(context, tool_call_json, iteration_state, stop_key):
                yield tool_event
        else:
            iteration_state.has_final_answer = True
            # ä¿å­˜åŠ©æ‰‹å›å¤
            if not self._is_stopped(stop_key):
                # æ„å»ºåŠ©æ‰‹æ¶ˆæ¯çš„å…ƒæ•°æ®
                ai_message_metadata = {}
                if request.model_id:
                    ai_message_metadata["model_id"] = request.model_id
                if request.knowledge_base_ids:
                    ai_message_metadata["knowledge_base_ids"] = request.knowledge_base_ids
                if request.use_web_search:
                    ai_message_metadata["web_search"] = True
                
                # æ¸…ç†å†…å®¹
                sanitized_content = self._sanitize_content(iteration_state.collected_content)
                
                await self.conversation_manager.add_assistant_message(
                    request, context.conversation_id, 
                    sanitized_content,
                    metadata=ai_message_metadata,
                    thinking=iteration_state.collected_thinking,
                    tool_calls=iteration_state.collected_tool_calls
                )

    def _process_tool_call(self, tool_call_data: str) -> str:
        """å¤„ç†å·¥å…·è°ƒç”¨æ•°æ®ï¼Œæ·»åŠ ID - å•å·¥å…·è°ƒç”¨æ¨¡å¼"""
        try:
            # è§£æå•ä¸ªå·¥å…·è°ƒç”¨å¯¹è±¡
            tool_call = json.loads(tool_call_data)
            
            # éªŒè¯æ ¼å¼
            if not isinstance(tool_call, dict) or "tool_name" not in tool_call:
                logger.error(f"å·¥å…·è°ƒç”¨æ ¼å¼ä¸æ­£ç¡®: {tool_call}")
                return tool_call_data
            
            # æ·»åŠ ID
            tool_call["id"] = f"{tool_call['tool_name']}_{uuid.uuid4().hex}"
            
            # åŒ…è£…ä¸ºæ•°ç»„æ ¼å¼ï¼ˆä¿æŒä¸åç»­å¤„ç†çš„å…¼å®¹æ€§ï¼‰
            return json.dumps([tool_call])
            
        except (json.JSONDecodeError, KeyError, TypeError) as e:
            logger.error(f"å¤„ç†å·¥å…·è°ƒç”¨æ•°æ®å¤±è´¥: {str(e)}, åŸå§‹æ•°æ®: {tool_call_data[:200]}...")
            return tool_call_data

    async def _handle_tool_call(self, context: ChatContext, tool_call_json: str, 
                              iteration_state: ChatIteration, stop_key: Optional[str]) -> AsyncGenerator[StreamEvent, None]:
        """å¤„ç†å·¥å…·è°ƒç”¨"""
        if self._is_stopped(stop_key):
            return
        
        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        tool_results = await self._execute_tool_calls(tool_call_json)
        
        # å‘ç”¨æˆ·æ˜¾ç¤ºå·¥å…·è°ƒç”¨ç»“æœ
        for result in tool_results:
            if self._is_stopped(stop_key):
                return
            yield StreamEvent(type=result.type, data=result.data)
        
        # æ”¶é›†å·¥å…·è°ƒç”¨ä¿¡æ¯
        try:
            for result in tool_results:
                if result.type == EventType.TOOL_RESULT:
                    # æ·±åº¦åºåˆ—åŒ–resultæ•°æ®ï¼Œç¡®ä¿æ²¡æœ‰ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
                    result_data = result.data.get("result", {})
                    serialized_result = self._deep_serialize_for_json(result_data)
                    
                    iteration_state.collected_tool_calls.append({
                        "id": result.data.get("id", ""),
                        "name": result.data.get("name", ""),
                        "arguments": result.data.get("arguments", {}),
                        "result": serialized_result,
                        "error": result.data.get("error", "")
                    })
        except Exception as e:
            logger.warning(f"å¤„ç†å·¥å…·è°ƒç”¨æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        
        # æ„å»ºè§‚å¯Ÿç»“æœå¹¶æ·»åŠ åˆ°ä¸Šä¸‹æ–‡ - ä½¿ç”¨assistantè§’è‰²è€Œä¸æ˜¯toolè§’è‰²
        observation = self._build_observation_message(tool_results)
        
        # ä¸ºäº†å…¼å®¹OpenAI APIï¼Œæˆ‘ä»¬å°†å·¥å…·è°ƒç”¨ç»“æœä½œä¸ºassistantæ¶ˆæ¯æ·»åŠ 
        # è€Œä¸æ˜¯ä½¿ç”¨toolè§’è‰²ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ReActæ¨¡å¼è€ŒéFunction Calling
        assistant_message = {
            "role": "assistant", 
            "content": f"å·¥å…·è°ƒç”¨ç»“æœï¼š\n{observation}" if observation else "å·¥å…·è°ƒç”¨å®Œæˆï¼Œä½†æ²¡æœ‰è¿”å›ç»“æœã€‚"
        }
        
        context.messages.append(assistant_message)

    async def _send_references(self, request: ChatRequest, context: ChatContext) -> AsyncGenerator[StreamEvent, None]:
        """å‘é€å¼•ç”¨ä¿¡æ¯"""
        # çŸ¥è¯†åº“å¼•ç”¨
        if request.knowledge_base_ids and hasattr(self.context_builder, 'knowledge_results') and self.context_builder.knowledge_results:
            references = self._build_knowledge_references(self.context_builder.knowledge_results)
            if references:
                yield StreamEvent(type="reference", data=references)
        
        # ç½‘ç»œæœç´¢å¼•ç”¨ï¼ˆæš‚æ—¶æ³¨é‡Šï¼‰
        # if request.use_web_search and hasattr(self.context_builder, 'search_results') and self.context_builder.search_results:
        #     web_references = self._build_web_references(self.context_builder.search_results)
        #     if web_references:
        #         yield StreamEvent(type="web_reference", data=web_references)

    def _build_knowledge_references(self, knowledge_results: List[Dict]) -> str:
        """æ„å»ºçŸ¥è¯†åº“å¼•ç”¨"""
        unique_sources = set()
        references = "\n\nå‚è€ƒæ¥æº:\n\n"
        
        count = 1
        for result in knowledge_results:
            metadata = result.get("metadata", {})
            source = metadata.get("source", "æœªçŸ¥æ¥æº")
            kb_info = result.get("source_knowledge_base", {})
            kb_name = kb_info.get("name", "æœªçŸ¥çŸ¥è¯†åº“")
            
            key = (source, kb_name)
            if key not in unique_sources:
                unique_sources.add(key)
                references += f"[{count}] {source} (çŸ¥è¯†åº“: {kb_name})\n\n"
                count += 1
        
        return references

    def _is_stopped(self, stop_key: Optional[str]) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ”¶åˆ°åœæ­¢ä¿¡å·"""
        if not stop_key:
            return False
        from ..api.routes.chat import _stop_signals
        return _stop_signals.get(stop_key, False)

    async def _handle_stop_signal(self, request: ChatRequest, context: ChatContext, iteration_state: ChatIteration):
        """å¤„ç†åœæ­¢ä¿¡å·ï¼Œä¿å­˜å½“å‰çŠ¶æ€"""
        try:
            if not context.conversation_id:
                return
            
            # ä¿å­˜å½“å‰å·²æ”¶é›†çš„å†…å®¹
            if iteration_state.collected_content or iteration_state.collected_thinking:
                # æ„å»ºåŠ©æ‰‹æ¶ˆæ¯çš„å…ƒæ•°æ®
                ai_message_metadata = {}
                if request.model_id:
                    ai_message_metadata["model_id"] = request.model_id
                if request.knowledge_base_ids:
                    ai_message_metadata["knowledge_base_ids"] = request.knowledge_base_ids
                if request.use_web_search:
                    ai_message_metadata["web_search"] = True
                
                # æ¸…ç†å†…å®¹
                sanitized_content = self._sanitize_content(iteration_state.collected_content or "")
                
                # ä¿å­˜åŠ©æ‰‹å›å¤ï¼ˆå³ä½¿ä¸å®Œæ•´ï¼‰
                await self.conversation_manager.add_assistant_message(
                    request, context.conversation_id, 
                    sanitized_content,
                    metadata=ai_message_metadata,
                    thinking=iteration_state.collected_thinking,
                    tool_calls=iteration_state.collected_tool_calls
                )
                
                logger.info(f"å·²ä¿å­˜abortæ—¶çš„ä¼šè¯çŠ¶æ€: {context.conversation_id}")
                
        except Exception as e:
            logger.error(f"ä¿å­˜abortçŠ¶æ€å¤±è´¥: {str(e)}")

    async def _get_user_provider(self, user_id: str, model_id: Optional[str] = None):
        """è·å–ç”¨æˆ·ç‰¹å®šçš„Provider"""
        if not user_id:
            return self.provider, model_id or self.settings.LLM_MODEL_NAME
        
        try:
            # è·å–æ‰€æœ‰ç”¨æˆ·é…ç½®
            user_configs = await self.user_llm_config_service.list_configs(user_id)
            
            user_config = None
            
            if model_id:
                # å¦‚æœæŒ‡å®šäº†model_idï¼ŒæŸ¥æ‰¾åŒ¹é…çš„é…ç½®
                for config in user_configs:
                    if config.model_name == model_id:
                        user_config = config
                        break
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„é…ç½®ï¼Œå°è¯•æ ¹æ®æ¨¡å‹åç§°æ¨æ–­provider
                if not user_config:
                    logger.info(f"æœªæ‰¾åˆ°æ¨¡å‹ {model_id} çš„é…ç½®ï¼Œå°è¯•æ¨æ–­provider")
                    inferred_provider = self._infer_provider_from_model(model_id)
                    
                    if inferred_provider:
                        # æŸ¥æ‰¾è¯¥providerçš„ä»»ä½•é…ç½®ä½œä¸ºåŸºç¡€
                        base_config = None
                        for config in user_configs:
                            if config.provider == inferred_provider:
                                base_config = config
                                break
                        
                        if base_config:
                            # ä½¿ç”¨åŸºç¡€é…ç½®ä½†æ›¿æ¢æ¨¡å‹åç§°
                            logger.info(f"ä½¿ç”¨ {inferred_provider} çš„åŸºç¡€é…ç½®ï¼Œæ¨¡å‹: {model_id}")
                            provider_params = self._get_provider_params_from_config(base_config)
                            user_provider = self._create_provider(inferred_provider, provider_params)
                            return user_provider, model_id
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
            if not user_config:
                for config in user_configs:
                    if config.is_default:
                        user_config = config
                        break
                
                # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªé…ç½®
                if not user_config and user_configs:
                    user_config = user_configs[0]
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ç”¨æˆ·é…ç½®ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤
            if not user_config:
                logger.info(f"ç”¨æˆ· {user_id} æ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤")
                return self.provider, model_id or self.settings.LLM_MODEL_NAME
            
            # æ ¹æ®ç”¨æˆ·é…ç½®åˆ›å»ºProvider
            provider_params = self._get_provider_params_from_config(user_config)
            user_provider = self._create_provider(user_config.provider, provider_params)
            
            # å¦‚æœæŒ‡å®šäº†model_idä¸”ä¸é…ç½®ä¸åŒï¼Œä½¿ç”¨æŒ‡å®šçš„model_id
            final_model = model_id if model_id else user_config.model_name
            
            logger.info(f"ä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰LLMé…ç½®: {user_config.provider} - {final_model}")
            return user_provider, final_model
            
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·Providerå¤±è´¥: {str(e)}, ä½¿ç”¨é»˜è®¤é…ç½®")
            return self.provider, model_id or self.settings.LLM_MODEL_NAME
    
    def _infer_provider_from_model(self, model_name: str) -> Optional[str]:
        """æ ¹æ®æ¨¡å‹åç§°æ¨æ–­provider"""
        model_lower = model_name.lower()
        
        if "deepseek" in model_lower:
            return "deepseek"
        elif "gpt" in model_lower or "o1" in model_lower:
            return "openai"
        elif "gemini" in model_lower:
            return "gemini"
        elif "claude" in model_lower:
            return "anthropic"
        elif "llama" in model_lower or "qwen" in model_lower:
            return "ollama"
        
        return None
    
    def _create_provider(self, provider_type: str, provider_params: Dict[str, Any]):
        """åˆ›å»ºProviderå®ä¾‹"""
        if provider_type == "deepseek":
            from ..lib.providers.deepseek import DeepSeekProvider
            return DeepSeekProvider(**provider_params)
        elif provider_type in ["openai", "azure"]:
            from ..lib.providers.openai import OpenAIProvider
            return OpenAIProvider(**provider_params)
        elif provider_type == "gemini":
            from ..lib.providers.gemini import GeminiProvider
            return GeminiProvider(**provider_params)
        elif provider_type in ["ollama", "local"]:
            from ..lib.providers.ollama import OllamaProvider
            return OllamaProvider(**provider_params)
        else:
            logger.warning(f"ä¸æ”¯æŒçš„æä¾›è€…ç±»å‹: {provider_type}")
            raise ValueError(f"ä¸æ”¯æŒçš„æä¾›è€…ç±»å‹: {provider_type}")

    def _get_provider_params_from_config(self, config) -> Dict[str, Any]:
        """ä»ç”¨æˆ·é…ç½®ä¸­æå–Providerå‚æ•°"""
        params = {}
        
        # åªè¿”å›Provideræ„é€ å‡½æ•°éœ€è¦çš„å‚æ•°
        if hasattr(config, 'api_key') and config.api_key:
            params["api_key"] = config.api_key
        elif config.provider in ["openai", "deepseek", "azure", "gemini", "anthropic"]:
            # å¦‚æœéœ€è¦APIå¯†é’¥ä½†æ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨ç³»ç»Ÿé…ç½®
            from ..core.config import get_settings
            settings = get_settings()
            if config.provider == "openai" and settings.OPENAI_API_KEY:
                params["api_key"] = settings.OPENAI_API_KEY
            elif config.provider == "deepseek" and settings.DEEPSEEK_API_KEY:
                params["api_key"] = settings.DEEPSEEK_API_KEY
            elif config.provider == "azure" and settings.AZURE_API_KEY:
                params["api_key"] = settings.AZURE_API_KEY
            elif config.provider == "gemini" and settings.GEMINI_API_KEY:
                params["api_key"] = settings.GEMINI_API_KEY
            elif config.provider == "anthropic" and settings.ANTHROPIC_API_KEY:
                params["api_key"] = settings.ANTHROPIC_API_KEY
            else:
                # å¦‚æœæ²¡æœ‰ç³»ç»Ÿé…ç½®çš„APIå¯†é’¥ï¼Œä½¿ç”¨é»˜è®¤å€¼é¿å…åˆå§‹åŒ–å¤±è´¥
                params["api_key"] = "default-key"
                logger.warning(f"Provider {config.provider} ç¼ºå°‘APIå¯†é’¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        
        if hasattr(config, 'base_url') and config.base_url:
            params["base_url"] = config.base_url
            
        return params

    async def _execute_tool_calls(self, tool_call_json: str) -> List[ModelEvent]:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶è¿”å›ç»“æœäº‹ä»¶"""
        results = []
        
        try:
            # è§£æå·¥å…·è°ƒç”¨JSON
            tool_calls = json.loads(tool_call_json)
            
            # æ ‡å‡†åŒ–å·¥å…·è°ƒç”¨æ ¼å¼
            normalized_tool_calls = self._normalize_tool_calls(tool_calls)
            
            if not normalized_tool_calls:
                logger.warning(f"å·¥å…·è°ƒç”¨æ ¼å¼ä¸æ­£ç¡®: {tool_call_json[:200]}...")
                error_tool_data = {
                    "id": "format_error",
                    "name": "format_validator",
                    "arguments": {},
                    "result": {"content": [{"type": "text", "text": get_message(MessageKeys.BAD_REQUEST)}]},
                    "error": get_message(MessageKeys.BAD_REQUEST)
                }
                results.append(ModelEvent(EventType.TOOL_RESULT, error_tool_data))
                return results
            
            # åˆ›å»ºå·¥å…·è°ƒç”¨ä»»åŠ¡åˆ—è¡¨
            tool_tasks = []
            for tool_call in normalized_tool_calls:
                action_id = tool_call.get("id")
                action_name = tool_call.get("tool_name")
                action_input = tool_call.get("arguments", {})
                
                if not action_name:
                    continue
                
                # åˆ›å»ºä»»åŠ¡ï¼Œä½¿ç”¨MCP Serviceçš„å·¥å…·è°ƒç”¨æ–¹æ³•
                task = asyncio.create_task(
                    self._safe_call_tool(action_name, action_input)
                )
                tool_tasks.append((task, action_name, action_input, action_id))
            
            # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
            for task, action_name, action_input, action_id in tool_tasks:
                try:
                    result = await task
                    
                    # åºåˆ—åŒ–ç»“æœ
                    json_result = self._serialize_call_tool_result(result)
                    
                    # åˆ›å»ºå·¥å…·è°ƒç”¨äº‹ä»¶
                    tool_data = {
                        "id": action_id,
                        "name": action_name,
                        "arguments": action_input,
                        "result": json_result
                    }
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
                    has_error = False
                    error_message = ""
                    
                    # æ£€æŸ¥å¤šç§é”™è¯¯æ ¼å¼
                    if isinstance(result, dict) and "error" in result:
                        has_error = True
                        error_message = result["error"]
                    elif hasattr(result, "isError") and result.isError:
                        has_error = True
                        if hasattr(result, "message"):
                            error_message = result.message
                        else:
                            error_message = get_message(MessageKeys.MCP_TOOL_CALL_FAILED, error="å·¥å…·æ‰§è¡Œå‡ºé”™")
                    elif isinstance(json_result, dict) and json_result.get("isError"):
                        has_error = True
                        error_message = get_message(MessageKeys.MCP_TOOL_CALL_FAILED, error="å·¥å…·æ‰§è¡Œå¤±è´¥")
                    
                    if has_error:
                        tool_data["error"] = error_message
                    
                    # æ·»åŠ å·¥å…·è°ƒç”¨äº‹ä»¶
                    results.append(ModelEvent(EventType.TOOL_RESULT, tool_data))
                    
                except Exception as e:
                    logger.error(f"å·¥å…· '{action_name}' æ‰§è¡Œå¼‚å¸¸: {str(e)}", exc_info=True)
                    # åˆ›å»ºé”™è¯¯çš„å·¥å…·ç»“æœäº‹ä»¶ï¼Œè€Œä¸æ˜¯ERRORäº‹ä»¶
                    error_tool_data = {
                        "id": action_id,
                        "name": action_name,
                        "arguments": action_input,
                        "result": {"content": [{"type": "text", "text": f"æ‰§è¡Œå¤±è´¥: {str(e)}"}]},
                        "error": get_message(MessageKeys.MCP_TOOL_CALL_FAILED, error=str(e))
                    }
                    results.append(ModelEvent(EventType.TOOL_RESULT, error_tool_data))
            
        except json.JSONDecodeError as e:
            logger.error(f"å·¥å…·è°ƒç”¨JSONè§£æå¤±è´¥: {str(e)}, æ•°æ®: {tool_call_json[:200]}...")
            # è¿”å›è§£æé”™è¯¯çš„å·¥å…·ç»“æœ
            error_tool_data = {
                "id": "parse_error",
                "name": "json_parser",
                "arguments": {},
                "result": {"content": [{"type": "text", "text": f"JSONè§£æå¤±è´¥: {str(e)}"}]},
                "error": get_message(MessageKeys.BAD_REQUEST)
            }
            results.append(ModelEvent(EventType.TOOL_RESULT, error_tool_data))
        
        return results
    
    def _build_observation_message(self, tool_results: List[ModelEvent]) -> str:
        """ä»å·¥å…·è°ƒç”¨ç»“æœæ„å»ºè§‚å¯Ÿæ¶ˆæ¯"""
        observations = []
        
        for event in tool_results:
            if event.type == EventType.TOOL_RESULT:
                tool_name = event.data.get("name", "unknown_tool")
                result = event.data.get("result", {})
                
                # æå–æ–‡æœ¬å†…å®¹
                observation_text = ""
                if isinstance(result, dict):
                    content = result.get("content", [])
                    if isinstance(content, list):
                        text_parts = []
                        for item in content:
                            if isinstance(item, dict) and item.get("type") == "text":
                                text_parts.append(item.get("text", ""))
                        observation_text = "\n".join(text_parts)
                    else:
                        observation_text = str(content)
                else:
                    observation_text = str(result)
                
                observations.append(f"Observation for {tool_name}: {observation_text}")
            
            elif event.type == EventType.ERROR:
                error_msg = event.data.get("error", "Unknown error")
                observations.append(f"Error: {error_msg}")
        
        return "\n\n".join(observations)

    def _normalize_tool_calls(self, tool_calls: Union[Dict, List]) -> List[Dict]:
        """æ ‡å‡†åŒ–å·¥å…·è°ƒç”¨æ ¼å¼"""
        normalized_calls = []
        
        # å¤„ç†åˆ—è¡¨æƒ…å†µ
        if isinstance(tool_calls, list):
            for item in tool_calls:
                if isinstance(item, dict) and item.get("tool_name"):
                    normalized_calls.append({
                        "id": item.get("id"),
                        "tool_name": item.get("tool_name"),
                        "arguments": item.get("arguments", {})
                    })
        # å¤„ç†å•ä¸ªå·¥å…·è°ƒç”¨æƒ…å†µ
        elif isinstance(tool_calls, dict) and tool_calls.get("tool_name"):
            normalized_calls.append({
                "id": tool_calls.get("id"),
                "tool_name": tool_calls.get("tool_name"),
                "arguments": tool_calls.get("arguments", {})
            })
            
        return normalized_calls
    
    async def _safe_call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Any:
        """å®‰å…¨åœ°è°ƒç”¨å·¥å…·ï¼Œå¤„ç†æ‰€æœ‰å¼‚å¸¸"""
        try:
            # é€šè¿‡MCP Serviceè°ƒç”¨å·¥å…·
            result = await self.mcp_service.call_tool(self.current_user.id, tool_name, arguments)
            return result
        except Exception as e:
            # ä¸å¤„ç†å¼‚å¸¸ï¼Œè®©è°ƒç”¨è€…å¤„ç†
            raise

    def _serialize_call_tool_result(self, result: Any) -> Dict[str, Any]:
        """å°†CallToolResultå¯¹è±¡åºåˆ—åŒ–ä¸ºå¯JSONåŒ–çš„å­—å…¸"""
        # å¤„ç†Noneå€¼
        if result is None:
            return {"content": "æ— ç»“æœ"}
        
        # å¤„ç†CallToolResultå¯¹è±¡
        if hasattr(result, "content") and hasattr(result, "isError"):
            serialized = {
                "isError": result.isError,
                "content": []
            }
            
            # å¤„ç†contentå±æ€§
            if result.content:
                if isinstance(result.content, list):
                    for item in result.content:
                        if hasattr(item, "text"):
                            # TextContentå¯¹è±¡
                            serialized["content"].append({
                                "type": "text",
                                "text": item.text
                            })
                        elif hasattr(item, "url"):
                            # ImageContentå¯¹è±¡
                            serialized["content"].append({
                                "type": "image",
                                "url": item.url
                            })
                        elif isinstance(item, dict):
                            # å·²ç»æ˜¯å­—å…¸
                            serialized["content"].append(item)
                        else:
                            # å…¶ä»–ç±»å‹
                            serialized["content"].append({
                                "type": "text",
                                "text": str(item)
                            })
                else:
                    # éåˆ—è¡¨å†…å®¹
                    serialized["content"] = [{
                        "type": "text",
                        "text": str(result.content)
                    }]
            
            return serialized
        # å¤„ç†åˆ—è¡¨
        elif isinstance(result, list):
            # åˆ—è¡¨éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œç¡®ä¿æ¯ä¸ªå…ƒç´ éƒ½å¯åºåˆ—åŒ–
            content_list = []
            for item in result:
                if isinstance(item, dict):
                    content_list.append(item)
                elif isinstance(item, str):
                    content_list.append({"type": "text", "text": item})
                else:
                    content_list.append({"type": "text", "text": str(item)})
            return {"content": content_list}
        # å¤„ç†å­—å…¸
        elif isinstance(result, dict):
            # å·²ç»æ˜¯å­—å…¸
            return result
        # å¤„ç†å­—ç¬¦ä¸²
        elif isinstance(result, str):
            try:
                # å°è¯•è§£æä¸ºJSON
                parsed = json.loads(result)
                return parsed if isinstance(parsed, dict) else {"content": parsed}
            except:
                # å¦‚æœä¸æ˜¯æœ‰æ•ˆçš„JSONï¼Œè¿”å›æ–‡æœ¬å†…å®¹
                return {"content": [{"type": "text", "text": result}]}
        # å¤„ç†å…¶ä»–ç±»å‹
        else:
            # å…¶ä»–ç±»å‹ï¼Œè½¬ä¸ºå­—ç¬¦ä¸²
            return {"content": [{"type": "text", "text": str(result)}]}

    def format_stream_event(self, event: StreamEvent) -> str:
        """æ ¼å¼åŒ–æµäº‹ä»¶ä¸ºæ–‡æœ¬"""
        return json.dumps(event.dict()) + "\n"

    async def prepare_chat_context(
        self,
        message: str,
        history: List[Dict[str, str]],
        knowledge_context: Optional[str] = None,
        web_search_context: Optional[str] = None,
        tools_info: Optional[str] = None
    ) -> List[Dict[str, str]]:
        """å‡†å¤‡èŠå¤©ä¸Šä¸‹æ–‡ï¼Œæ•´åˆçŸ¥è¯†åº“å’Œå·¥å…·ä¿¡æ¯ - å…¼å®¹æ€§æ–¹æ³•"""
        # è¿™ä¸ªæ–¹æ³•ä¿æŒå‘åå…¼å®¹
        context_parts = []
        
        # ç½‘ç»œæœç´¢ç»“æœä¼˜å…ˆ
        if web_search_context:
            context_parts.append(web_search_context)
            
        if knowledge_context:
            context_parts.append(knowledge_context)
            
        if tools_info:
            context_parts.append(tools_info)
        
        # å¢å¼ºç”¨æˆ·æ¶ˆæ¯ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡
        user_message = message
        if context_parts:
            context = "\n\n".join(context_parts)
            user_message = f"{context}\n\nç”¨æˆ·é—®é¢˜: {message}"
        
        # å‡†å¤‡æ¶ˆæ¯å†å² - åŒ…å«æ‰€æœ‰å†å²æ¶ˆæ¯
        messages = []
        if history:
            # æ·»åŠ æ‰€æœ‰å†å²æ¶ˆæ¯
            messages.extend(history)
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯ä½œä¸ºæœ€åä¸€æ¡
        messages.append({"role": "user", "content": user_message})
        
        return messages

    def _sanitize_content(self, content: str) -> str:
        """æ¸…ç†æ¶ˆæ¯å†…å®¹ï¼Œç§»é™¤ä»»ä½•å¯èƒ½å¯¼è‡´JSONè§£æé”™è¯¯çš„æ§åˆ¶å­—ç¬¦"""
        if not content:
            return ""
            
        try:
            # å°è¯•ç¼–ç å’Œè§£ç å†…å®¹ï¼Œè¿™å°†å¸®åŠ©è¿‡æ»¤éæ³•å­—ç¬¦
            sanitized = content.encode('utf-8', errors='ignore').decode('utf-8')
            
            # æ›¿æ¢JSONä¸æ”¯æŒçš„æ§åˆ¶å­—ç¬¦
            import re
            # ç§»é™¤ASCIIæ§åˆ¶å­—ç¬¦(0-31)ï¼Œä½†ä¿ç•™åˆ¶è¡¨ç¬¦(9)ã€æ¢è¡Œç¬¦(10)å’Œå›è½¦ç¬¦(13)
            sanitized = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', sanitized)
            
            # æµ‹è¯•èƒ½å¦ä½œä¸ºJSONåºåˆ—åŒ–ï¼Œè¿™æœ‰åŠ©äºæ•è·æ·±å±‚æ¬¡çš„é—®é¢˜
            json.dumps({"content": sanitized})
            
            return sanitized
        except Exception as e:
            # å¦‚æœå‡ºç°é—®é¢˜ï¼Œè®°å½•é”™è¯¯å¹¶è¿”å›ç®€åŒ–ç‰ˆæœ¬
            logger.error(f"å†…å®¹æ¸…ç†å¤±è´¥: {str(e)}")
            # è¿”å›ç®€åŒ–çš„å†…å®¹ï¼Œç¡®ä¿è‡³å°‘èƒ½ä¿å­˜åŸºæœ¬æ–‡æœ¬
            return content.replace('\u0000', '').strip()
    
    def _deep_serialize_for_json(self, obj: Any) -> Any:
        """æ·±åº¦åºåˆ—åŒ–å¯¹è±¡ï¼Œç¡®ä¿æ‰€æœ‰å†…å®¹éƒ½å¯ä»¥JSONåºåˆ—åŒ–"""
        if obj is None:
            return None
        
        # å¤„ç†åŸºæœ¬ç±»å‹
        if isinstance(obj, (str, int, float, bool)):
            return obj
        
        # å¤„ç†åˆ—è¡¨
        if isinstance(obj, list):
            return [self._deep_serialize_for_json(item) for item in obj]
        
        # å¤„ç†å­—å…¸
        if isinstance(obj, dict):
            return {key: self._deep_serialize_for_json(value) for key, value in obj.items()}
        
        # å¤„ç†å…·æœ‰textå±æ€§çš„å¯¹è±¡ï¼ˆå¦‚TextContentï¼‰
        if hasattr(obj, 'text'):
            return {
                "type": "text",
                "text": str(obj.text)
            }
        
        # å¤„ç†å…·æœ‰urlå±æ€§çš„å¯¹è±¡ï¼ˆå¦‚ImageContentï¼‰
        if hasattr(obj, 'url'):
            return {
                "type": "image", 
                "url": str(obj.url)
            }
        
        # å¤„ç†å…¶ä»–å¯¹è±¡ï¼Œå°è¯•è½¬æ¢ä¸ºå­—å…¸
        if hasattr(obj, '__dict__'):
            try:
                return self._deep_serialize_for_json(obj.__dict__)
            except:
                return str(obj)
        
        # æœ€åçš„å›é€€ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
        return str(obj)