"""
çŸ¥è¯†åº“æœåŠ¡ - æä¾›çŸ¥è¯†åº“ç®¡ç†å’ŒæŸ¥è¯¢åŠŸèƒ½
"""
import os
import uuid
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Union, BinaryIO
from enum import Enum
from sqlalchemy.ext.asyncio import AsyncSession
import numpy as np

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.schema import TextNode
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb

from ..core.config import get_settings, get_embedding_model, normalize_embedding
from ..core.logging import get_logger
from ..core.service import BaseService
from ..core.constants import KnowledgeConstants
from ..core.errors import (
    NotFoundException, ServiceException, AuthorizationException, 
    ConflictException, ValidationException, KnowledgeBaseNotFoundException,
    DocumentNotFoundException
)
from ..domain.models.user import User, UserRole
from ..domain.models.knowledge_base import KnowledgeBase, KnowledgeFile, KnowledgeBaseType, KnowledgeBaseStatus, FileStatus
from ..repositories.knowledge import KnowledgeBaseRepository, KnowledgeFileRepository
from ..lib.knowledge.document import load_documents_from_file
from ..lib.knowledge.builder import KnowledgeBaseBuilder
from ..lib.knowledge.config import KnowledgeBaseConfig

logger = get_logger(__name__)

class KnowledgeService(BaseService[KnowledgeBase, KnowledgeBaseRepository]):
    """çŸ¥è¯†åº“æœåŠ¡ï¼Œæä¾›ç»Ÿä¸€çš„çŸ¥è¯†åº“ç®¡ç†æ¥å£"""

    def __init__(self, session: AsyncSession):
        """åˆå§‹åŒ–çŸ¥è¯†åº“æœåŠ¡"""
        kb_repository = KnowledgeBaseRepository(session)
        super().__init__(kb_repository)
        
        self.settings = get_settings()
        self.session = session
        self.file_repo = KnowledgeFileRepository(session)
        self._embedding_model = None
        # ä½ å¯ä»¥åœ¨è¿™é‡Œä¸ºæ¯ä¸ªçŸ¥è¯†åº“é…ç½®ä¸€ä¸ªBuilderï¼Œæˆ–è€…æŒ‰éœ€åˆ›å»º
        self.builders: Dict[str, KnowledgeBaseBuilder] = {} 
        logger.info("çŸ¥è¯†åº“æœåŠ¡åˆå§‹åŒ–")

    def get_entity_name(self) -> str:
        """è·å–å®ä½“åç§°"""
        return "çŸ¥è¯†åº“"

    async def _get_builder_for_kb(self, kb_id: str) -> KnowledgeBaseBuilder:
        """è·å–æˆ–åˆ›å»ºæŒ‡å®šçŸ¥è¯†åº“çš„Builderå®ä¾‹"""
        kb = await self.repository.get_by_id(kb_id)
        if not kb:
            raise NotFoundException(f"çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨")
        
        # ä¿®å¤ï¼šå°†Pathå¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        vectors_path = self.repository.get_knowledge_base_storage_path(kb_id) / "vectors"
        
        kb_config = KnowledgeBaseConfig(
            collection_name=f"kb_{kb_id}_collection",
            embedding_model=kb.embedding_model,
            db_path=str(vectors_path)  # ä¿®å¤ï¼šè½¬æ¢ä¸ºå­—ç¬¦ä¸²
        )
        # ç¼“å­˜Builderå®ä¾‹ï¼Œé¿å…é‡å¤åˆ›å»º
        if kb_id not in self.builders:
            self.builders[kb_id] = KnowledgeBaseBuilder(config=kb_config)
        return self.builders[kb_id]
        
    def get_embedding_model(self):
        """è·å–åµŒå…¥æ¨¡å‹"""
        if self._embedding_model is None:
            self._embedding_model = get_embedding_model()
        return self._embedding_model

    async def list_knowledge_bases(self, current_user: Optional[User] = None) -> List[Dict[str, Any]]:
        """åˆ—å‡ºçŸ¥è¯†åº“"""
        is_admin = current_user and current_user.role == UserRole.ADMIN
        user_id = current_user.id if current_user else None
        
        # è·å–ç”¨æˆ·å¯è®¿é—®çš„çŸ¥è¯†åº“
        knowledge_bases = await self.repository.get_accessible_knowledge_bases(user_id, is_admin)
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        return [kb.to_dict() for kb in knowledge_bases]

    async def get_knowledge_base(self, kb_id: str, current_user: Optional[User] = None) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“è¯¦æƒ…"""
        # å…ˆå°è¯•ç›´æ¥è·å–
        kb = await self.repository.get_by_id(kb_id)
        if not kb:
            raise NotFoundException(f"çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨")
            
        # æ£€æŸ¥è®¿é—®æƒé™
        if not await self._can_access_knowledge_base(kb, current_user):
            raise AuthorizationException(f"æ— æƒè®¿é—®çŸ¥è¯†åº“ {kb_id}")
            
        # è½¬æ¢ä¸ºå­—å…¸è¿”å›
        return kb.to_dict()
        
    async def _can_access_knowledge_base(self, kb: KnowledgeBase, user: Optional[User]) -> bool:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å¯ä»¥è®¿é—®çŸ¥è¯†åº“"""
        # ç®¡ç†å‘˜å¯ä»¥è®¿é—®æ‰€æœ‰çŸ¥è¯†åº“
        if user and user.role == UserRole.ADMIN:
            return True
            
        # å…¬å¼€çŸ¥è¯†åº“ä»»ä½•äººå¯è®¿é—®
        if kb.kb_type == KnowledgeBaseType.PUBLIC.value:
            return True
            
        # æœªç™»å½•ç”¨æˆ·ä¸èƒ½è®¿é—®éå…¬å¼€çŸ¥è¯†åº“
        if not user:
            return False
            
        # çŸ¥è¯†åº“æ‰€æœ‰è€…å¯ä»¥è®¿é—®
        if kb.owner_id == user.id:
            return True
            
        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å…±äº«ç»™äº†ç”¨æˆ·
        if kb.kb_type == KnowledgeBaseType.SHARED.value:
            # è¿™ä¸ªæ–¹æ³•è¿˜ä¸å­˜åœ¨ï¼Œéœ€è¦åœ¨Repositoryä¸­å®ç°
            # shared_users = await self.repository.get_shared_users(kb.id)
            # return user.id in shared_users
            return False  # ä¸´æ—¶è¿”å›Falseï¼Œå¾…å®ç°å…±äº«åŠŸèƒ½
            
        return False

    async def create_knowledge_base(self, name: str, description: str = "", embedding_model: Optional[str] = None, 
                             kb_type: KnowledgeBaseType = KnowledgeBaseType.PERSONAL, 
                             current_user: Optional[User] = None, is_public: bool = False) -> Dict[str, Any]:
        """åˆ›å»ºæ–°çš„çŸ¥è¯†åº“"""
        # æ£€æŸ¥çŸ¥è¯†åº“åç§°æ˜¯å¦å·²å­˜åœ¨
        existing_kb = await self.repository.find_by_name(name, current_user.id if current_user else None)
        if existing_kb:
            raise ConflictException(f"çŸ¥è¯†åº“ '{name}' å·²å­˜åœ¨")
            
        # ç”Ÿæˆå”¯ä¸€ID
        kb_id = str(uuid.uuid4())
        
        # å¤„ç†is_publicå‚æ•°ï¼Œå¦‚æœis_publicä¸ºTrueï¼Œkb_typeè®¾ä¸ºPUBLIC
        if is_public:
            kb_type = KnowledgeBaseType.PUBLIC
        
        # åˆ›å»ºçŸ¥è¯†åº“æ•°æ® - ä¸åŒ…å«is_publicå­—æ®µ
        kb_data = {
            "id": kb_id,
            "name": name,
            "description": description,
            "owner_id": current_user.id if current_user else None,
            "embedding_model": embedding_model or self.get_embedding_model().model_name,
            "status": KnowledgeBaseStatus.ACTIVE.value,
            "kb_type": kb_type.value if kb_type else KnowledgeBaseType.PERSONAL.value
        }
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        created_kb = await self.repository.create(kb_data)
        
        # åˆ›å»ºçŸ¥è¯†åº“çš„å­˜å‚¨ç›®å½•
        self.repository.get_knowledge_base_storage_path(kb_id)
        
        return created_kb.to_dict()

    async def update_knowledge_base(self, kb_id: str, name: Optional[str] = None, 
                             description: Optional[str] = None, kb_type: Optional[KnowledgeBaseType] = None,
                             current_user: Optional[User] = None, is_public: Optional[bool] = None) -> Dict[str, Any]:
        """æ›´æ–°çŸ¥è¯†åº“ä¿¡æ¯"""
        # è·å–çŸ¥è¯†åº“
        kb = await self.repository.get_by_id(kb_id)
        if not kb:
            raise NotFoundException(f"çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨")
            
        # æ£€æŸ¥æƒé™
        if not self._can_modify_knowledge_base(kb, current_user):
            raise AuthorizationException("æ— æƒä¿®æ”¹æ­¤çŸ¥è¯†åº“")
            
        # å‡†å¤‡æ›´æ–°æ•°æ®
        update_data = {}
        
        # å¦‚æœè¦æ›´æ–°åç§°ï¼Œæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if name and name != kb.name:
            existing_kb = await self.repository.find_by_name(name, kb.owner_id)
            if existing_kb and existing_kb.id != kb_id:
                raise ConflictException(f"çŸ¥è¯†åº“åç§° '{name}' å·²è¢«ä½¿ç”¨")
            update_data["name"] = name
            
        # æ›´æ–°å…¶ä»–å­—æ®µ
        if description is not None:
            update_data["description"] = description
            
        if kb_type is not None:
            update_data["kb_type"] = kb_type.value
        
        # æ ¹æ®is_publicå‚æ•°æ›´æ–°çŸ¥è¯†åº“ç±»å‹    
        if is_public is not None:
            # å¦‚æœè®¾ç½®ä¸ºå…¬å¼€ï¼Œåˆ™ä½¿ç”¨PUBLICç±»å‹
            if is_public:
                update_data["kb_type"] = KnowledgeBaseType.PUBLIC.value
            # å¦‚æœå–æ¶ˆå…¬å¼€ï¼Œä¸”å½“å‰æ˜¯å…¬å¼€ç±»å‹ï¼Œåˆ™æ”¹ä¸ºä¸ªäººç±»å‹
            elif kb.kb_type == KnowledgeBaseType.PUBLIC.value:
                update_data["kb_type"] = KnowledgeBaseType.PERSONAL.value
        
        if update_data:
            update_data["updated_at"] = datetime.now()
            # ä¿å­˜æ›´æ–°
            updated_kb = await self.repository.update(kb_id, update_data)
            return updated_kb.to_dict()
        
        return kb.to_dict()
        
    def _can_modify_knowledge_base(self, kb: KnowledgeBase, user: Optional[User]) -> bool:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å¯ä»¥ä¿®æ”¹çŸ¥è¯†åº“"""
        if not user:
            return False
            
        # ç®¡ç†å‘˜å¯ä»¥ä¿®æ”¹æ‰€æœ‰çŸ¥è¯†åº“
        if user.role == UserRole.ADMIN:
            return True
            
        # çŸ¥è¯†åº“æ‰€æœ‰è€…å¯ä»¥ä¿®æ”¹
        if kb.owner_id == user.id:
            return True
            
        return False

    async def delete_knowledge_base(self, kb_id: str, current_user: Optional[User] = None) -> bool:
        """åˆ é™¤çŸ¥è¯†åº“"""
        # è·å–çŸ¥è¯†åº“
        kb = await self.repository.get_by_id(kb_id)
        if not kb:
            raise NotFoundException(f"çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨")
            
        # æ£€æŸ¥æƒé™
        if not self._can_modify_knowledge_base(kb, current_user):
            raise AuthorizationException("æ— æƒåˆ é™¤æ­¤çŸ¥è¯†åº“")
            
        # æ‰§è¡Œåˆ é™¤æ“ä½œï¼ˆåŒ…æ‹¬æ–‡ä»¶ç³»ç»Ÿæ¸…ç†ï¼‰
        await self.repository.delete_knowledge_base_files(kb_id)
        success = await self.repository.delete(kb_id)
        
        return success

    async def share_knowledge_base(self, kb_id: str, user_id: str, current_user: Optional[User] = None) -> bool:
        """å…±äº«çŸ¥è¯†åº“ç»™æŒ‡å®šç”¨æˆ·"""
        # è·å–çŸ¥è¯†åº“
        kb = await self.repository.get_by_id(kb_id)
        if not kb:
            raise NotFoundException(f"çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨")
            
        # æ£€æŸ¥æƒé™
        if not self._can_modify_knowledge_base(kb, current_user):
            raise AuthorizationException("æ— æƒå…±äº«æ­¤çŸ¥è¯†åº“")
            
        # å¦‚æœçŸ¥è¯†åº“ç±»å‹ä¸æ˜¯å…±äº«ç±»å‹ï¼Œå…ˆæ›´æ–°ç±»å‹
        if kb.kb_type != KnowledgeBaseType.SHARED.value:
            await self.repository.update(kb_id, {"kb_type": KnowledgeBaseType.SHARED.value})
            
        # æ‰§è¡Œå…±äº«æ“ä½œï¼ˆè¿™ä¸ªæ–¹æ³•éœ€è¦åœ¨Repositoryä¸­å®ç°ï¼‰
        # return await self.repository.share_knowledge_base(kb_id, user_id)
        return True  # ä¸´æ—¶è¿”å›ï¼Œå¾…å®ç°

    async def unshare_knowledge_base(self, kb_id: str, user_id: str, current_user: Optional[User] = None) -> bool:
        """å–æ¶ˆä¸æŒ‡å®šç”¨æˆ·å…±äº«çŸ¥è¯†åº“"""
        # è·å–çŸ¥è¯†åº“
        kb = await self.repository.get_by_id(kb_id)
        if not kb:
            raise NotFoundException(f"çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨")
            
        # æ£€æŸ¥æƒé™
        if not self._can_modify_knowledge_base(kb, current_user):
            raise AuthorizationException("æ— æƒå–æ¶ˆå…±äº«æ­¤çŸ¥è¯†åº“")
            
        # æ‰§è¡Œå–æ¶ˆå…±äº«æ“ä½œï¼ˆè¿™ä¸ªæ–¹æ³•éœ€è¦åœ¨Repositoryä¸­å®ç°ï¼‰
        # return await self.repository.unshare_knowledge_base(kb_id, user_id)
        return True  # ä¸´æ—¶è¿”å›ï¼Œå¾…å®ç°

    async def get_shared_users(self, kb_id: str, current_user: Optional[User] = None) -> List[str]:
        """è·å–çŸ¥è¯†åº“å…±äº«çš„ç”¨æˆ·åˆ—è¡¨"""
        # è·å–çŸ¥è¯†åº“
        kb = await self.repository.get_by_id(kb_id)
        if not kb:
            raise NotFoundException(f"çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨")
            
        # æ£€æŸ¥æƒé™
        if not self._can_modify_knowledge_base(kb, current_user):
            raise AuthorizationException("æ— æƒæŸ¥çœ‹å…±äº«ç”¨æˆ·åˆ—è¡¨")
            
        # è·å–å…±äº«ç”¨æˆ·åˆ—è¡¨ï¼ˆè¿™ä¸ªæ–¹æ³•éœ€è¦åœ¨Repositoryä¸­å®ç°ï¼‰
        # return await self.repository.get_shared_users(kb_id)
        return []  # ä¸´æ—¶è¿”å›ï¼Œå¾…å®ç°

    async def upload_file(self, kb_id: str, file_name: str, file_content: bytes, 
                   file_type: Optional[str] = None, current_user: Optional[User] = None, 
                   use_simple_chunking: bool = False) -> Dict[str, Any]:
        """ä¸Šä¼ æ–‡ä»¶åˆ°çŸ¥è¯†åº“
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            file_name: æ–‡ä»¶å
            file_content: æ–‡ä»¶å†…å®¹
            file_type: æ–‡ä»¶ç±»å‹
            current_user: å½“å‰ç”¨æˆ·
            use_simple_chunking: æ˜¯å¦ä½¿ç”¨ç®€å•åˆ†å—ï¼ˆTrue=ä½¿ç”¨SentenceSplitterï¼ŒFalse=ä½¿ç”¨ç»“æ„åŒ–åˆ†å—ï¼‰
        """
        # è·å–çŸ¥è¯†åº“
        kb = await self.repository.get_by_id(kb_id)
        if not kb:
            raise NotFoundException(f"çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨")
            
        # æ£€æŸ¥æƒé™
        if not self._can_modify_knowledge_base(kb, current_user):
            raise AuthorizationException("æ— æƒå‘æ­¤çŸ¥è¯†åº“ä¸Šä¼ æ–‡ä»¶")
            
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        existing_file = await self.file_repo.find_by_name(kb_id, file_name)
        if existing_file:
            raise ConflictException(f"æ–‡ä»¶ '{file_name}' å·²å­˜åœ¨")
            
        # ä¿å­˜æ–‡ä»¶ï¼ˆæ•°æ®åº“è®°å½• + æ–‡ä»¶ç³»ç»Ÿï¼‰
        file_record = await self.file_repo.save_file(file_content, kb_id, file_name, file_type)
        
        # è‡ªåŠ¨ä¸ºä¸Šä¼ çš„æ–‡ä»¶åˆ›å»ºç´¢å¼•
        try:
            chunking_method = "ç®€å•åˆ†å—" if use_simple_chunking else "ç»“æ„åŒ–åˆ†å—"
            logger.info(f"å¼€å§‹ä¸ºæ–‡ä»¶ {file_name} åˆ›å»ºç´¢å¼•ï¼Œä½¿ç”¨{chunking_method}")
            index_result = await self.add_or_update_file_to_kb(kb_id, file_record.id, current_user, use_simple_chunking)
            logger.info(f"æ–‡ä»¶ {file_name} ç´¢å¼•åˆ›å»ºå®Œæˆï¼Œä½¿ç”¨{chunking_method}")
            
            # è¿”å›åŒ…å«ç´¢å¼•ä¿¡æ¯çš„ç»“æœ
            result = file_record.to_dict()
            result["index_status"] = index_result.get("status", "UNKNOWN")
            result["nodes_indexed"] = index_result.get("nodes_indexed", 0)
            result["chunking_method"] = chunking_method
            return result
            
        except Exception as e:
            logger.error(f"ä¸ºæ–‡ä»¶ {file_name} åˆ›å»ºç´¢å¼•å¤±è´¥: {str(e)}")
            # å³ä½¿ç´¢å¼•åˆ›å»ºå¤±è´¥ï¼Œæ–‡ä»¶ä¸Šä¼ ä»ç„¶æˆåŠŸ
            result = file_record.to_dict()
            result["index_status"] = "ERROR"
            result["index_error"] = str(e)
            result["chunking_method"] = chunking_method
            return result

    async def delete_file(self, kb_id: str, file_name: str, current_user: Optional[User] = None) -> bool:
        """ä»çŸ¥è¯†åº“åˆ é™¤æ–‡ä»¶"""
        # è·å–çŸ¥è¯†åº“
        kb = await self.repository.get_by_id(kb_id)
        if not kb:
            raise NotFoundException(f"çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨")
            
        # æ£€æŸ¥æƒé™
        if not self._can_modify_knowledge_base(kb, current_user):
            raise AuthorizationException("æ— æƒä»æ­¤çŸ¥è¯†åº“åˆ é™¤æ–‡ä»¶")
            
        # æŸ¥æ‰¾æ–‡ä»¶
        file_record = await self.file_repo.find_by_name(kb_id, file_name)
        if not file_record:
            raise NotFoundException(f"æ–‡ä»¶ '{file_name}' ä¸å­˜åœ¨")
        
        try:
            # 1. å…ˆä»å‘é‡å­˜å‚¨ä¸­åˆ é™¤ç›¸å…³æ–‡æ¡£ï¼ˆä¸åœ¨äº‹åŠ¡ä¸­ï¼Œé¿å…é”å®šï¼‰
            vector_cleanup_success = await self._remove_file_from_vector_store(kb_id, file_record.id)
            
            # 2. åˆ é™¤æ–‡ä»¶ï¼ˆæ•°æ®åº“è®°å½• + æ–‡ä»¶ç³»ç»Ÿï¼‰
            success = await self.file_repo.delete_file(file_record.id)
            
            if success:
                logger.info(f"æ–‡ä»¶ {file_name} åˆ é™¤æˆåŠŸ")
                
                # 3. æ›´æ–°çŸ¥è¯†åº“æ–‡æ¡£æ•°é‡
                remaining_files = await self.file_repo.find_by_knowledge_base(kb_id)
                indexed_files = [f for f in remaining_files if f.status == "indexed"]
                total_chunks = sum(f.chunk_count or 0 for f in indexed_files)
                
                await self.repository.update(kb_id, {"document_count": total_chunks})
                
                # å¦‚æœå‘é‡æ¸…ç†å¤±è´¥ï¼Œè®°å½•è­¦å‘Šä½†ä¸å½±å“åˆ é™¤ç»“æœ
                if not vector_cleanup_success:
                    logger.warning(f"æ–‡ä»¶ {file_name} çš„å‘é‡æ•°æ®æ¸…ç†å¯èƒ½ä¸å®Œæ•´ï¼Œå»ºè®®é‡å»ºç´¢å¼•")
                
                return True
            else:
                # æ–‡ä»¶åˆ é™¤å¤±è´¥
                raise ServiceException(f"åˆ é™¤æ–‡ä»¶è®°å½•å¤±è´¥")
                
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {str(e)}")
            raise ServiceException(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")

    async def _remove_file_from_vector_store(self, kb_id: str, file_id: str) -> bool:
        """ä»å‘é‡å­˜å‚¨ä¸­åˆ é™¤æŒ‡å®šæ–‡ä»¶çš„æ‰€æœ‰æ–‡æ¡£ï¼Œæ”¹è¿›æ¸…ç†é€»è¾‘"""
        try:
            # è·å–å‘é‡å­˜å‚¨è·¯å¾„
            vectors_path = self._get_vector_store_path(kb_id)
            
            if not Path(vectors_path).exists():
                logger.info(f"å‘é‡å­˜å‚¨ä¸å­˜åœ¨ï¼Œè·³è¿‡æ¸…ç†: {vectors_path}")
                return True
            
            # è¿æ¥åˆ°ChromaDB
            client = chromadb.PersistentClient(path=str(vectors_path))
            collection_name = f"kb_{kb_id}_collection"
            
            try:
                collection = client.get_collection(collection_name)
                
                # å°è¯•å¤šä¸ªå¯èƒ½çš„æ–‡ä»¶IDå­—æ®µåï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„æŸ¥è¯¢
                possible_fields = ["file_ref_id", "file_id", "source_file_id"]
                total_deleted = 0
                
                for field_name in possible_fields:
                    try:
                        # æŸ¥è¯¢åŒ…å«æŒ‡å®šfile_idçš„æ‰€æœ‰æ–‡æ¡£
                        results = collection.get(
                            where={field_name: file_id},
                            include=["metadatas", "documents"]
                        )
                        
                        if results and results['ids']:
                            # åˆ é™¤è¿™äº›æ–‡æ¡£
                            collection.delete(ids=results['ids'])
                            total_deleted += len(results['ids'])
                            logger.info(f"ä»å‘é‡å­˜å‚¨ä¸­åˆ é™¤äº† {len(results['ids'])} ä¸ªæ–‡æ¡£å— (å­—æ®µ: {field_name}, æ–‡ä»¶ID: {file_id})")
                    except Exception as field_error:
                        logger.debug(f"æŸ¥è¯¢å­—æ®µ {field_name} å¤±è´¥: {field_error}")
                        continue
                
                # é¢å¤–æ¸…ç†ï¼šæŸ¥æ‰¾å¯èƒ½é—æ¼çš„æ–‡æ¡£ï¼ˆé€šè¿‡æ–‡æ¡£å†…å®¹åŒ¹é…ï¼‰
                try:
                    # è·å–æ‰€æœ‰æ–‡æ¡£çš„å…ƒæ•°æ®
                    all_docs = collection.get(include=["metadatas"])
                    orphaned_ids = []
                    
                    for i, metadata in enumerate(all_docs.get('metadatas', [])):
                        if metadata:
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡ä»¶IDçš„ä»»ä½•å¼•ç”¨
                            metadata_str = str(metadata).lower()
                            if file_id.lower() in metadata_str:
                                orphaned_ids.append(all_docs['ids'][i])
                    
                    if orphaned_ids:
                        collection.delete(ids=orphaned_ids)
                        total_deleted += len(orphaned_ids)
                        logger.info(f"æ¸…ç†äº† {len(orphaned_ids)} ä¸ªå¯èƒ½é—æ¼çš„æ–‡æ¡£")
                        
                except Exception as cleanup_error:
                    logger.warning(f"é¢å¤–æ¸…ç†è¿‡ç¨‹å‡ºé”™: {cleanup_error}")
                
                if total_deleted == 0:
                    logger.info(f"å‘é‡å­˜å‚¨ä¸­æœªæ‰¾åˆ°æ–‡ä»¶IDä¸º {file_id} çš„æ–‡æ¡£")
                else:
                    logger.info(f"æ€»å…±åˆ é™¤äº† {total_deleted} ä¸ªæ–‡æ¡£å—")
                
                return True
                
            except Exception as e:
                logger.warning(f"è®¿é—®å‘é‡å­˜å‚¨é›†åˆæ—¶å‡ºé”™: {e}")
                return False  # å‘é‡æ¸…ç†å¤±è´¥
                
        except Exception as e:
            logger.error(f"æ¸…ç†å‘é‡å­˜å‚¨æ—¶å‡ºé”™: {e}")
            return False  # å‘é‡æ¸…ç†å¤±è´¥

    async def list_files(self, kb_id: str, current_user: Optional[User] = None) -> List[Dict[str, Any]]:
        """è·å–çŸ¥è¯†åº“ä¸­çš„æ–‡ä»¶åˆ—è¡¨"""
        # è·å–çŸ¥è¯†åº“
        kb = await self.repository.get_by_id(kb_id)
        if not kb:
            raise NotFoundException(f"çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨")
            
        # æ£€æŸ¥æƒé™
        if not await self._can_access_knowledge_base(kb, current_user):
            raise AuthorizationException("æ— æƒè®¿é—®æ­¤çŸ¥è¯†åº“")
            
        # è·å–æ–‡ä»¶åˆ—è¡¨
        files = await self.file_repo.find_by_knowledge_base(kb_id)
        
        return [file.to_dict() for file in files]
    
    async def add_or_update_file_to_kb(self, kb_id: str, file_id: str, current_user: Optional[User] = None, use_simple_chunking: bool = False) -> Dict[str, Any]:
        chunking_method = "ç®€å•åˆ†å—" if use_simple_chunking else "ç»“æ„åŒ–åˆ†å—"
        logger.info(f"Service: å¼€å§‹å¤„ç†çŸ¥è¯†åº“ {kb_id} ä¸­çš„æ–‡ä»¶ {file_id}ï¼Œä½¿ç”¨{chunking_method}")
        # 1. Serviceå±‚è´Ÿè´£ä¸šåŠ¡é€»è¾‘ï¼šè·å–å¯¹è±¡ã€æƒé™æ£€æŸ¥ã€çŠ¶æ€æ›´æ–°
        kb = await self.repository.get_by_id(kb_id)
        if not self._can_modify_knowledge_base(kb, current_user):
            raise AuthorizationException("æ— æƒä¿®æ”¹æ­¤çŸ¥è¯†åº“")
        
        file_model = await self.file_repo.get_by_id(file_id) # FileModelæ˜¯ä½ æ•°æ®åº“ä¸­çš„æ–‡ä»¶å¯¹è±¡
        if not file_model:
            raise NotFoundException(f"æ–‡ä»¶ {file_id} ä¸å­˜åœ¨")
        
        if file_model.knowledge_base_id != kb_id:
            raise ValidationException(f"æ–‡ä»¶ {file_id} ä¸å±äºçŸ¥è¯†åº“ {kb_id}")

        file_model.status = FileStatus.PROCESSING.value # æ›´æ–°çŠ¶æ€
        await self.file_repo.update(file_model.id, {"status": file_model.status})

        try:
            # 2. è·å–Builderå®ä¾‹
            builder = await self._get_builder_for_kb(kb_id) # å‡è®¾è¿™ä¸ªæ–¹æ³•èƒ½æ­£ç¡®è¿”å›Builderå®ä¾‹
            
            # 3. Serviceå±‚ç›´æ¥è°ƒç”¨Builderçš„å°è£…å¥½çš„æ–¹æ³•
            result = builder.index_single_file(
                file_path=file_model.file_path,
                file_database_id=str(file_model.id),
                knowledge_base_id=kb_id,
                source_filename_for_metadata=file_model.file_name,
                use_simple_chunking=use_simple_chunking
            )
            
            # 4. Serviceå±‚æ ¹æ®Builderè¿”å›çš„ç»“æœï¼Œæ›´æ–°æ•°æ®åº“çŠ¶æ€
            if result["status"] == "SUCCESS":
                file_model.status = FileStatus.INDEXED.value
                file_model.chunk_count = result["nodes_indexed"]
                logger.info(f"Service: æ–‡ä»¶ {file_id} å¤„ç†æˆåŠŸï¼Œä½¿ç”¨{chunking_method}ã€‚")
                
                # æ›´æ–°æ–‡ä»¶çŠ¶æ€
                await self.file_repo.update(file_model.id, {"status": file_model.status, "chunk_count": file_model.chunk_count})
                
                # ğŸ”¥ æ–°å¢ï¼šæ›´æ–°çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯
                await self._update_knowledge_base_stats(kb_id)
                logger.info(f"Service: çŸ¥è¯†åº“ {kb_id} ç»Ÿè®¡ä¿¡æ¯å·²æ›´æ–°")
                
            else:
                file_model.status = FileStatus.ERROR.value
                logger.error(f"Service: æ–‡ä»¶ {file_id} å¤„ç†å¤±è´¥ã€‚åŸå› : {result['message']}")
                await self.file_repo.update(file_model.id, {"status": file_model.status, "chunk_count": file_model.chunk_count})
            
            return result

        except Exception as e:
            # ä¿®å¤ï¼šåœ¨å¼‚å¸¸æ—¶æ›´æ–°æ–‡ä»¶çŠ¶æ€ä¸ºERROR
            logger.error(f"Service: æ–‡ä»¶ {file_id} å¤„ç†å¼‚å¸¸: {str(e)}", exc_info=True)
            try:
                await self.file_repo.update(file_model.id, {"status": FileStatus.ERROR.value})
            except Exception as update_error:
                logger.error(f"æ›´æ–°æ–‡ä»¶çŠ¶æ€å¤±è´¥: {str(update_error)}")
            raise ServiceException(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")

    async def _update_knowledge_base_stats(self, kb_id: str):
        """æ›´æ–°çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # è·å–æ‰€æœ‰å·²ç´¢å¼•çš„æ–‡ä»¶
            files = await self.file_repo.find_by_knowledge_base(kb_id)
            indexed_files = [f for f in files if f.status == FileStatus.INDEXED.value]
            
            # è®¡ç®—æ€»å—æ•°
            total_chunks = sum(f.chunk_count or 0 for f in indexed_files)
            
            # æ›´æ–°çŸ¥è¯†åº“ç»Ÿè®¡
            await self.repository.update(kb_id, {
                "file_count": len(files),
                "document_count": total_chunks
            })
            
            logger.info(f"çŸ¥è¯†åº“ {kb_id} ç»Ÿè®¡æ›´æ–°: æ–‡ä»¶æ•°={len(files)}, æ–‡æ¡£å—æ•°={total_chunks}")
            
        except Exception as e:
            logger.error(f"æ›´æ–°çŸ¥è¯†åº“ç»Ÿè®¡å¤±è´¥: {str(e)}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹

    async def rebuild_index(self, kb_id: str, current_user: Optional[User] = None) -> bool:
        """é‡å»ºçŸ¥è¯†åº“ç´¢å¼•"""
        # è·å–çŸ¥è¯†åº“
        kb = await self.repository.get_by_id(kb_id)
        if not kb:
            raise NotFoundException(f"çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨")
            
        # æ£€æŸ¥æƒé™
        if not self._can_modify_knowledge_base(kb, current_user):
            raise AuthorizationException("æ— æƒé‡å»ºæ­¤çŸ¥è¯†åº“ç´¢å¼•")
            
        # è·å–çŸ¥è¯†åº“å­˜å‚¨è·¯å¾„
        kb_path = self.repository.get_knowledge_base_storage_path(kb_id)
        files_dir = kb_path / "files"
        vectors_dir = kb_path / "vectors"
        
        # æ›´æ–°çŸ¥è¯†åº“çŠ¶æ€
        kb.status = KnowledgeBaseStatus.BUILDING.value
        await self.repository.update(kb_id, {"status": kb.status})
        
        try:
            # è·å–æ–‡ä»¶åˆ—è¡¨
            files = await self.file_repo.find_by_knowledge_base(kb_id)
            
            # å¦‚æœæ²¡æœ‰æ–‡ä»¶ï¼Œæ— éœ€åˆ›å»ºç´¢å¼•
            if not files:
                kb.status = KnowledgeBaseStatus.ACTIVE.value
                kb.document_count = 0
                await self.repository.update(kb_id, {"status": kb.status, "document_count": kb.document_count})
                return True
                
            # åˆ›å»ºå‘é‡å­˜å‚¨
            client = chromadb.PersistentClient(path=str(vectors_dir))
            collection_name = f"kb_{kb_id}_collection"
            collection = client.get_or_create_collection(collection_name)
            vector_store = ChromaVectorStore(chroma_collection=collection)
            
            # ä½¿ç”¨LlamaIndexçš„SentenceSplitteræ›¿ä»£RecursiveCharacterTextSplitter
            text_splitter = SentenceSplitter(
                chunk_size=KnowledgeConstants.DEFAULT_CHUNK_SIZE,  # ä½¿ç”¨å¸¸é‡
                chunk_overlap=KnowledgeConstants.DEFAULT_CHUNK_OVERLAP  # ä½¿ç”¨å¸¸é‡
            )
            
            # è·å–é¢„å…ˆé…ç½®çš„åµŒå…¥æ¨¡å‹
            embed_model = self.get_embedding_model()
            
            # å¤„ç†æ¯ä¸ªæ–‡ä»¶
            total_nodes = 0
            nodes_list = []  # å­˜å‚¨æ‰€æœ‰çš„æ–‡æ¡£èŠ‚ç‚¹
            for file in files:
                # æ›´æ–°æ–‡ä»¶çŠ¶æ€
                file.status = FileStatus.PROCESSING.value
                await self.file_repo.update(file.id, {"status": file.status})
                
                try:
                    # è¯»å–æ–‡ä»¶å†…å®¹
                    file_path = Path(file.file_path)
                    if not file_path.exists():
                        logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                        file.status = FileStatus.ERROR.value
                        await self.file_repo.update(file.id, {"status": file.status})
                        continue
                        
                    # ä½¿ç”¨LlamaIndexçš„æ–‡æ¡£åŠ è½½å™¨å¤„ç†æ–‡ä»¶
                    try:
                        # æ”¹è¿›æ–‡æ¡£åŠ è½½é€»è¾‘ï¼Œå¢åŠ é”™è¯¯å¤„ç†
                        documents = SimpleDirectoryReader(
                            input_files=[str(file_path)],
                            # æ·»åŠ æ–‡ä»¶ç±»å‹æ”¯æŒé…ç½®
                            file_extractor={
                                ".docx": "default",
                                ".doc": "default", 
                                ".pdf": "default",
                                ".txt": "default",
                                ".md": "default"
                            },
                            # å¿½ç•¥éšè—æ–‡ä»¶å’Œä¸´æ—¶æ–‡ä»¶
                            exclude_hidden=True,
                            # é€’å½’å¤„ç†
                            recursive=False
                        ).load_data()
                        
                        # æ£€æŸ¥æ˜¯å¦æˆåŠŸåŠ è½½æ–‡æ¡£
                        if not documents:
                            logger.warning(f"æ–‡ä»¶ {file_path} åŠ è½½åä¸ºç©º")
                            file.status = FileStatus.ERROR.value
                            await self.file_repo.update(file.id, {"status": file.status})
                            continue
                            
                        # æ£€æŸ¥æ–‡æ¡£å†…å®¹æ˜¯å¦æœ‰æ•ˆ
                        document = documents[0]
                        if not document.text or len(document.text.strip()) == 0:
                            logger.warning(f"æ–‡ä»¶ {file_path} å†…å®¹ä¸ºç©º")
                            file.status = FileStatus.ERROR.value
                            await self.file_repo.update(file.id, {"status": file.status})
                            continue
                            
                        logger.info(f"æˆåŠŸåŠ è½½æ–‡ä»¶ {file_path}ï¼Œå†…å®¹é•¿åº¦: {len(document.text)}")
                        
                    except Exception as e:
                        logger.error(f"åŠ è½½æ–‡ä»¶ {file_path} å‡ºé”™: {str(e)}")
                        # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•åŠ è½½Wordæ–‡ä»¶
                        if file_path.suffix.lower() in ['.docx', '.doc']:
                            try:
                                logger.info(f"å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•åŠ è½½Wordæ–‡ä»¶: {file_path}")
                                from ..lib.knowledge.document import load_documents_from_file
                                backup_docs = load_documents_from_file(str(file_path))
                                if backup_docs and len(backup_docs) > 0:
                                    # ç¡®ä¿è¿”å›çš„æ˜¯Documentå¯¹è±¡åˆ—è¡¨
                                    if isinstance(backup_docs[0], str):
                                        # å¦‚æœè¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºDocumentå¯¹è±¡
                                        from llama_index.core import Document
                                        documents = [Document(text=backup_docs[0])]
                                    else:
                                        documents = backup_docs
                                    logger.info(f"å¤‡ç”¨æ–¹æ³•æˆåŠŸåŠ è½½Wordæ–‡ä»¶: {file_path}")
                                else:
                                    raise Exception("å¤‡ç”¨æ–¹æ³•ä¹Ÿæ— æ³•åŠ è½½æ–‡ä»¶")
                            except Exception as backup_error:
                                logger.error(f"å¤‡ç”¨æ–¹æ³•åŠ è½½Wordæ–‡ä»¶å¤±è´¥: {backup_error}")
                                file.status = FileStatus.ERROR.value
                                await self.file_repo.update(file.id, {"status": file.status})
                                continue
                        else:
                            file.status = FileStatus.ERROR.value
                            await self.file_repo.update(file.id, {"status": file.status})
                            continue
                        
                    # åˆ†å‰²æ–‡æ¡£
                    nodes = text_splitter.get_nodes_from_documents([documents[0]])
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    for node in nodes:
                        node.metadata = {
                            "source": file.file_name,
                            "file_id": file.id,
                            "knowledge_base_id": kb_id
                        }
                    
                    # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹
                    nodes_list.extend(nodes)
                            
                    # æ›´æ–°æ–‡ä»¶çŠ¶æ€å’Œå—æ•°é‡
                    file.status = FileStatus.INDEXED.value
                    file.chunk_count = len(nodes)
                    await self.file_repo.update(file.id, {"status": file.status, "chunk_count": file.chunk_count})
                    
                    total_nodes += len(nodes)
                    
                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶ {file.file_name} å‡ºé”™: {str(e)}")
                    file.status = FileStatus.ERROR.value
                    await self.file_repo.update(file.id, {"status": file.status})
            
            # åˆ›å»ºç´¢å¼•å¹¶æ’å…¥æ‰€æœ‰èŠ‚ç‚¹
            index = VectorStoreIndex.from_vector_store(
                vector_store,
                embed_model=embed_model
            )
            if nodes_list:
                index.insert_nodes(nodes_list)
            
            # æ›´æ–°çŸ¥è¯†åº“çŠ¶æ€å’Œæ–‡æ¡£æ•°é‡
            kb.status = KnowledgeBaseStatus.ACTIVE.value
            kb.document_count = total_nodes
            await self.repository.update(kb_id, {"status": kb.status, "document_count": kb.document_count})
            
            return True
            
        except Exception as e:
            logger.error(f"é‡å»ºçŸ¥è¯†åº“ {kb_id} ç´¢å¼•å‡ºé”™: {str(e)}")
            kb.status = KnowledgeBaseStatus.ERROR.value
            await self.repository.update(kb_id, {"status": kb.status})
            raise ServiceException(f"é‡å»ºç´¢å¼•å¤±è´¥: {str(e)}")

    async def rebuild_index2(self, kb_id: str, current_user: Optional[User] = None) -> bool:
        """é‡å»ºçŸ¥è¯†åº“ç´¢å¼•"""
        # è·å–çŸ¥è¯†åº“
        kb = await self.repository.get_by_id(kb_id)
        if not kb:
            raise NotFoundException(f"çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨")
            
        # æ£€æŸ¥æƒé™
        if not self._can_modify_knowledge_base(kb, current_user):
            raise AuthorizationException("æ— æƒé‡å»ºæ­¤çŸ¥è¯†åº“ç´¢å¼•")
        
        # æ›´æ–°çŸ¥è¯†åº“çŠ¶æ€
        kb.status = KnowledgeBaseStatus.BUILDING.value
        await self.repository.update(kb_id, {"status": kb.status})
        
        try:
            # è·å–æ–‡ä»¶åˆ—è¡¨
            files = await self.file_repo.find_by_knowledge_base(kb_id)
            
            # å¦‚æœæ²¡æœ‰æ–‡ä»¶ï¼Œæ— éœ€åˆ›å»ºç´¢å¼•
            if not files:
                kb.status = KnowledgeBaseStatus.ACTIVE.value
                kb.document_count = 0
                await self.repository.update(kb_id, {"status": kb.status, "document_count": kb.document_count})
                return True
           
            builder = await self._get_builder_for_kb(kb_id)
            total_nodes = 0
            for file in files:
                # æ›´æ–°æ–‡ä»¶çŠ¶æ€
                file.status = FileStatus.PROCESSING.value
                await self.file_repo.update(file.id, {"status": file.status})
                
                try:
                    result = builder.index_single_file(
                        file_path=file.file_path,
                        file_database_id=str(file.id),
                        knowledge_base_id=kb_id,
                        source_filename_for_metadata=file.file_name
                    )
                    if result["status"] == "SUCCESS":
                        total_nodes += result["nodes_indexed"]
                            
                    # æ›´æ–°æ–‡ä»¶çŠ¶æ€å’Œå—æ•°é‡
                    file.status = FileStatus.INDEXED.value
                    file.chunk_count = result["nodes_indexed"]
                    await self.file_repo.update(file.id, {"status": file.status, "chunk_count": file.chunk_count})                    
                    
                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶ {file.file_name} å‡ºé”™: {str(e)}")
                    file.status = FileStatus.ERROR.value
                    await self.file_repo.update(file.id, {"status": file.status})
            
            # æ›´æ–°çŸ¥è¯†åº“çŠ¶æ€å’Œæ–‡æ¡£æ•°é‡
            kb.status = KnowledgeBaseStatus.ACTIVE.value
            kb.document_count = total_nodes
            await self.repository.update(kb_id, {"status": kb.status, "document_count": kb.document_count})
            
            return True
            
        except Exception as e:
            logger.error(f"é‡å»ºçŸ¥è¯†åº“ {kb_id} ç´¢å¼•å‡ºé”™: {str(e)}")
            kb.status = KnowledgeBaseStatus.ERROR.value
            await self.repository.update(kb_id, {"status": kb.status})
            raise ServiceException(f"é‡å»ºç´¢å¼•å¤±è´¥: {str(e)}")
        

    async def query(self, kb_id: str, query_text: str, top_k: int = 5, 
             current_user: Optional[User] = None) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢çŸ¥è¯†åº“"""
        # è·å–çŸ¥è¯†åº“
        kb = await self.repository.get_by_id(kb_id)
        if not kb:
            raise NotFoundException(f"çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨")
            
        # æ£€æŸ¥æƒé™
        if not await self._can_access_knowledge_base(kb, current_user):
            raise AuthorizationException("æ— æƒæŸ¥è¯¢æ­¤çŸ¥è¯†åº“")
            
        # è·å–çŸ¥è¯†åº“å­˜å‚¨è·¯å¾„
        kb_path = self.repository.get_knowledge_base_storage_path(kb_id)
        vectors_dir = kb_path / "vectors"
        
        # æ£€æŸ¥å‘é‡å­˜å‚¨æ˜¯å¦å­˜åœ¨
        if not vectors_dir.exists() or not any(vectors_dir.iterdir()):
            raise ValidationException("çŸ¥è¯†åº“å°šæœªå»ºç«‹ç´¢å¼•")
            
        try:
            # ç›´æ¥ä½¿ç”¨ChromaDBè¿›è¡ŒæŸ¥è¯¢ï¼Œé¿å…LlamaIndexçš„å‘é‡å¤„ç†
            client = chromadb.PersistentClient(path=str(vectors_dir))
            collection_name = f"kb_{kb_id}_collection"
            collection = client.get_or_create_collection(collection_name)
            
            # è·å–åµŒå…¥æ¨¡å‹å¹¶ç”ŸæˆæŸ¥è¯¢å‘é‡
            embed_model = self.get_embedding_model()
            query_embedding = embed_model.get_text_embedding(query_text)
            
            # å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡
            normalized_query = normalize_embedding(query_embedding)
            
            # 1. é¦–å…ˆè¿›è¡Œå…³é”®è¯æœç´¢ï¼ŒæŸ¥æ‰¾åŒ…å«æŸ¥è¯¢è¯çš„æ–‡æ¡£
            keyword_results = []
            try:
                # è·å–æ‰€æœ‰æ–‡æ¡£è¿›è¡Œå…³é”®è¯åŒ¹é…
                all_docs = collection.get(include=['documents', 'metadatas'])
                
                for doc_id, doc, metadata in zip(all_docs['ids'], all_docs['documents'], all_docs['metadatas']):
                    # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦åŒ…å«æŸ¥è¯¢å…³é”®è¯ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
                    if query_text.lower() in doc.lower():
                        # è®¡ç®—å‘é‡ç›¸ä¼¼åº¦
                        doc_embedding = collection.get(ids=[doc_id], include=['embeddings'])['embeddings'][0]
                        distance = np.linalg.norm(np.array(normalized_query) - np.array(doc_embedding))
                        similarity_score = max(0, 1 - distance / 2)
                        
                        keyword_results.append({
                            "document": doc,
                            "metadata": metadata,
                            "score": similarity_score + 0.2,  # ç»™å…³é”®è¯åŒ¹é…åŠ æƒ
                            "match_type": "keyword"
                        })
                        
                logger.info(f"å…³é”®è¯æœç´¢æ‰¾åˆ° {len(keyword_results)} ä¸ªç»“æœ")
                        
            except Exception as e:
                logger.warning(f"å…³é”®è¯æœç´¢å¤±è´¥: {e}")
            
            # 2. è¿›è¡Œå‘é‡æœç´¢
            chroma_results = collection.query(
                query_embeddings=[normalized_query],
                n_results=top_k * 2,  # è·å–æ›´å¤šç»“æœç”¨äºåˆå¹¶
                include=['documents', 'metadatas', 'distances']
            )
            
            # è½¬æ¢å‘é‡æœç´¢ç»“æœæ ¼å¼
            vector_results = []
            if chroma_results['ids'][0]:
                for i, (doc_id, doc, metadata, distance) in enumerate(zip(
                    chroma_results['ids'][0],
                    chroma_results['documents'][0],
                    chroma_results['metadatas'][0],
                    chroma_results['distances'][0]
                )):
                    # å°†è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•° (1 - normalized_distance)
                    # å¯¹äºå½’ä¸€åŒ–å‘é‡ï¼Œè·ç¦»èŒƒå›´æ˜¯0-2ï¼Œæ‰€ä»¥ç›¸ä¼¼åº¦ = 1 - distance/2
                    similarity_score = max(0, 1 - distance / 2)
                    
                    vector_results.append({
                        "document": doc,
                        "metadata": metadata,
                        "score": similarity_score,
                        "match_type": "vector"
                    })
            
            # 3. åˆå¹¶å’Œå»é‡ç»“æœ
            all_results = []
            seen_docs = set()
            
            # é¦–å…ˆæ·»åŠ å…³é”®è¯åŒ¹é…çš„ç»“æœï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
            for result in keyword_results:
                doc_content = result["document"]
                if doc_content not in seen_docs:
                    all_results.append(result)
                    seen_docs.add(doc_content)
            
            # ç„¶åæ·»åŠ å‘é‡æœç´¢çš„ç»“æœ
            for result in vector_results:
                doc_content = result["document"]
                if doc_content not in seen_docs:
                    all_results.append(result)
                    seen_docs.add(doc_content)
            
            # 4. æŒ‰ç›¸ä¼¼åº¦æ’åº
            all_results.sort(key=lambda x: x.get("score", 0), reverse=True)
            
            # 5. é™åˆ¶ç»“æœæ•°é‡å¹¶ç§»é™¤match_typeå­—æ®µ
            final_results = []
            for result in all_results[:top_k]:
                final_result = {
                    "document": result["document"],
                    "metadata": result["metadata"],
                    "score": result["score"]
                }
                final_results.append(final_result)
            
            logger.info(f"æ··åˆæœç´¢è¿”å› {len(final_results)} ä¸ªç»“æœ")
            return final_results
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢çŸ¥è¯†åº“ {kb_id} å‡ºé”™: {str(e)}")
            raise ServiceException(f"æŸ¥è¯¢å¤±è´¥: {str(e)}")

    async def query_multiple(self, kb_ids: List[str], query_text: str, top_k: int = 5,
                      current_user: Optional[User] = None) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢å¤šä¸ªçŸ¥è¯†åº“"""
        all_results = []
        
        # æŸ¥è¯¢æ¯ä¸ªçŸ¥è¯†åº“
        for kb_id in kb_ids:
            try:
                # è·å–å•ä¸ªçŸ¥è¯†åº“çš„æŸ¥è¯¢ç»“æœ
                results = await self.query(kb_id, query_text, top_k, current_user)
                
                # æ·»åŠ çŸ¥è¯†åº“ä¿¡æ¯
                kb = await self.repository.get_by_id(kb_id)
                if kb:
                    for result in results:
                        result["source_knowledge_base"] = {
                            "id": kb.id,
                            "name": kb.name
                        }
                        
                all_results.extend(results)
            except Exception as e:
                logger.warning(f"æŸ¥è¯¢çŸ¥è¯†åº“ {kb_id} å‡ºé”™: {str(e)}")
                # ç»§ç»­å¤„ç†å…¶ä»–çŸ¥è¯†åº“
                
        # æŒ‰ç›¸å…³æ€§æ’åº
        all_results.sort(key=lambda x: x.get("score", 0), reverse=True)
        
        # é™åˆ¶ç»“æœæ•°é‡
        if top_k > 0 and len(all_results) > top_k:
            all_results = all_results[:top_k]
            
        return all_results

    def format_knowledge_results(self, results: List[Dict[str, Any]]) -> str:
        """å°†çŸ¥è¯†åº“ç»“æœæ ¼å¼åŒ–ä¸ºæ–‡æœ¬"""
        if not results:
            return ""
            
        formatted_text = "ä»¥ä¸‹æ˜¯ç›¸å…³å‚è€ƒä¿¡æ¯ï¼š\n\n"
        
        for i, result in enumerate(results, 1):
            content = result.get("document", "")
            metadata = result.get("metadata", {})
            source = metadata.get("source", "æœªçŸ¥æ¥æº")
            
            # ä» source_knowledge_base ä¸­è·å–çŸ¥è¯†åº“ä¿¡æ¯
            kb_info = result.get("source_knowledge_base", {})
            kb_name = kb_info.get("name", "æœªçŸ¥çŸ¥è¯†åº“")
            
            formatted_text += f"[{i}] æ¥æº: {source}ï¼ˆçŸ¥è¯†åº“:{kb_name}ï¼‰\n"
            formatted_text += f"{content}\n\n"
        
        return formatted_text 
    
    def _get_or_create_vector_store_index(self, kb_id: str) -> VectorStoreIndex:
        """
        ã€è¾…åŠ©å‡½æ•°ã€‘è·å–æˆ–åˆ›å»ºæŒ‡å®šçŸ¥è¯†åº“çš„LlamaIndex VectorStoreIndexã€‚
        """
        kb_storage_path = self.repository.get_knowledge_base_storage_path(kb_id)
        vectors_dir = kb_storage_path / "vectors"
        
        client = chromadb.PersistentClient(path=str(vectors_dir))
        collection_name = f"kb_{kb_id}_collection"
        collection = client.get_or_create_collection(collection_name)
        vector_store = ChromaVectorStore(chroma_collection=collection)
        embed_model = self.get_embedding_model()
        
        index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)
        return index

    def _get_vector_store_path(self, kb_id: str) -> str:
         # è·å–çŸ¥è¯†åº“å­˜å‚¨è·¯å¾„
        kb_path = self.repository.get_knowledge_base_storage_path(kb_id)
        vectors_dir = kb_path / "vectors"
        return vectors_dir

    